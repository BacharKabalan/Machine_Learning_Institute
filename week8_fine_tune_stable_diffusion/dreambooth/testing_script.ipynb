{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "347d5860-b062-4335-8ed9-7903e69a2f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'diffusers'...\n",
      "remote: Enumerating objects: 48201, done.\u001b[K\n",
      "remote: Counting objects: 100% (12545/12545), done.\u001b[K\n",
      "remote: Compressing objects: 100% (800/800), done.\u001b[K\n",
      "remote: Total 48201 (delta 12150), reused 11831 (delta 11710), pack-reused 35656\u001b[K\n",
      "Receiving objects: 100% (48201/48201), 31.25 MiB | 38.92 MiB/s, done.\n",
      "Resolving deltas: 100% (35936/35936), done.\n",
      "Processing ./diffusers\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/lib/python3/dist-packages (from diffusers==0.25.0.dev0) (4.6.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers==0.25.0.dev0) (3.9.0)\n",
      "Collecting huggingface-hub>=0.19.4 (from diffusers==0.25.0.dev0)\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers==0.25.0.dev0) (1.24.1)\n",
      "Collecting regex!=2019.12.17 (from diffusers==0.25.0.dev0)\n",
      "  Downloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m742.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers==0.25.0.dev0) (2.31.0)\n",
      "Collecting safetensors>=0.3.1 (from diffusers==0.25.0.dev0)\n",
      "  Downloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers==0.25.0.dev0) (9.3.0)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.19.4->diffusers==0.25.0.dev0)\n",
      "  Downloading fsspec-2023.12.2-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting tqdm>=4.42.1 (from huggingface-hub>=0.19.4->diffusers==0.25.0.dev0)\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->diffusers==0.25.0.dev0) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->diffusers==0.25.0.dev0) (4.4.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->diffusers==0.25.0.dev0) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.25.0.dev0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.25.0.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.25.0.dev0) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.25.0.dev0) (2022.12.7)\n",
      "Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.9/773.9 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.0/169.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: diffusers\n",
      "  Building wheel for diffusers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for diffusers: filename=diffusers-0.25.0.dev0-py3-none-any.whl size=1818207 sha256=866a150b7c0d5ec3c99acfdb59b08d0c4a1f427cb599f45c33ed552423a6361e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-7tlsgfpm/wheels/64/78/12/760ac642f0ef9ca736fa88da48b7461adddf1b24155a244ddc\n",
      "Successfully built diffusers\n",
      "Installing collected packages: tqdm, safetensors, regex, fsspec, huggingface-hub, diffusers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed diffusers-0.25.0.dev0 fsspec-2023.12.2 huggingface-hub-0.19.4 regex-2023.10.3 safetensors-0.4.1 tqdm-4.66.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting accelerate>=0.16.0 (from -r ./diffusers/examples/dreambooth/requirements.txt (line 1))\n",
      "  Downloading accelerate-0.25.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r ./diffusers/examples/dreambooth/requirements.txt (line 2)) (0.16.0+cu118)\n",
      "Collecting torchvision (from -r ./diffusers/examples/dreambooth/requirements.txt (line 2))\n",
      "  Downloading torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting transformers>=4.25.1 (from -r ./diffusers/examples/dreambooth/requirements.txt (line 3))\n",
      "  Downloading transformers-4.36.1-py3-none-any.whl.metadata (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting ftfy (from -r ./diffusers/examples/dreambooth/requirements.txt (line 4))\n",
      "  Downloading ftfy-6.1.3-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting tensorboard (from -r ./diffusers/examples/dreambooth/requirements.txt (line 5))\n",
      "  Downloading tensorboard-2.15.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.10/dist-packages (from -r ./diffusers/examples/dreambooth/requirements.txt (line 6)) (3.1.2)\n",
      "Collecting peft==0.7.0 (from -r ./diffusers/examples/dreambooth/requirements.txt (line 7))\n",
      "  Downloading peft-0.7.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r ./diffusers/examples/dreambooth/requirements.txt (line 7)) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r ./diffusers/examples/dreambooth/requirements.txt (line 7)) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r ./diffusers/examples/dreambooth/requirements.txt (line 7)) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r ./diffusers/examples/dreambooth/requirements.txt (line 7)) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r ./diffusers/examples/dreambooth/requirements.txt (line 7)) (2.1.0+cu118)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r ./diffusers/examples/dreambooth/requirements.txt (line 7)) (4.66.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r ./diffusers/examples/dreambooth/requirements.txt (line 7)) (0.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r ./diffusers/examples/dreambooth/requirements.txt (line 7)) (0.19.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->-r ./diffusers/examples/dreambooth/requirements.txt (line 2)) (2.31.0)\n",
      "Collecting torch>=1.13.0 (from peft==0.7.0->-r ./diffusers/examples/dreambooth/requirements.txt (line 7))\n",
      "  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->-r ./diffusers/examples/dreambooth/requirements.txt (line 2)) (9.3.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r ./diffusers/examples/dreambooth/requirements.txt (line 7)) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r ./diffusers/examples/dreambooth/requirements.txt (line 7)) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r ./diffusers/examples/dreambooth/requirements.txt (line 7)) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r ./diffusers/examples/dreambooth/requirements.txt (line 7)) (3.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r ./diffusers/examples/dreambooth/requirements.txt (line 7)) (2023.12.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13.0->peft==0.7.0->-r ./diffusers/examples/dreambooth/requirements.txt (line 7))\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13.0->peft==0.7.0->-r ./diffusers/examples/dreambooth/requirements.txt (line 7))\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13.0->peft==0.7.0->-r ./diffusers/examples/dreambooth/requirements.txt (line 7))\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13.0->peft==0.7.0->-r ./diffusers/examples/dreambooth/requirements.txt (line 7))\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13.0->peft==0.7.0->-r ./diffusers/examples/dreambooth/requirements.txt (line 7))\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13.0->peft==0.7.0->-r ./diffusers/examples/dreambooth/requirements.txt (line 7))\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13.0->peft==0.7.0->-r ./diffusers/examples/dreambooth/requirements.txt (line 7))\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13.0->peft==0.7.0->-r ./diffusers/examples/dreambooth/requirements.txt (line 7))\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13.0->peft==0.7.0->-r ./diffusers/examples/dreambooth/requirements.txt (line 7))\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch>=1.13.0->peft==0.7.0->-r ./diffusers/examples/dreambooth/requirements.txt (line 7))\n",
      "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13.0->peft==0.7.0->-r ./diffusers/examples/dreambooth/requirements.txt (line 7))\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r ./diffusers/examples/dreambooth/requirements.txt (line 7)) (2.1.0)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft==0.7.0->-r ./diffusers/examples/dreambooth/requirements.txt (line 7))\n",
      "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r ./diffusers/examples/dreambooth/requirements.txt (line 3)) (2023.10.3)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers>=4.25.1->-r ./diffusers/examples/dreambooth/requirements.txt (line 3))\n",
      "  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting wcwidth<0.3.0,>=0.2.12 (from ftfy->-r ./diffusers/examples/dreambooth/requirements.txt (line 4))\n",
      "  Downloading wcwidth-0.2.12-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard->-r ./diffusers/examples/dreambooth/requirements.txt (line 5))\n",
      "  Downloading absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard->-r ./diffusers/examples/dreambooth/requirements.txt (line 5))\n",
      "  Downloading grpcio-1.60.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard->-r ./diffusers/examples/dreambooth/requirements.txt (line 5))\n",
      "  Downloading google_auth-2.25.2-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard->-r ./diffusers/examples/dreambooth/requirements.txt (line 5))\n",
      "  Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard->-r ./diffusers/examples/dreambooth/requirements.txt (line 5))\n",
      "  Downloading Markdown-3.5.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting protobuf<4.24,>=3.19.6 (from tensorboard->-r ./diffusers/examples/dreambooth/requirements.txt (line 5))\n",
      "  Downloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (540 bytes)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r ./diffusers/examples/dreambooth/requirements.txt (line 5)) (68.2.2)\n",
      "Requirement already satisfied: six>1.9 in /usr/lib/python3/dist-packages (from tensorboard->-r ./diffusers/examples/dreambooth/requirements.txt (line 5)) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->-r ./diffusers/examples/dreambooth/requirements.txt (line 5))\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard->-r ./diffusers/examples/dreambooth/requirements.txt (line 5))\n",
      "  Downloading werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2->-r ./diffusers/examples/dreambooth/requirements.txt (line 6)) (2.1.2)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard->-r ./diffusers/examples/dreambooth/requirements.txt (line 5))\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard->-r ./diffusers/examples/dreambooth/requirements.txt (line 5))\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard->-r ./diffusers/examples/dreambooth/requirements.txt (line 5))\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<2,>=0.5->tensorboard->-r ./diffusers/examples/dreambooth/requirements.txt (line 5))\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r ./diffusers/examples/dreambooth/requirements.txt (line 2)) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r ./diffusers/examples/dreambooth/requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r ./diffusers/examples/dreambooth/requirements.txt (line 2)) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r ./diffusers/examples/dreambooth/requirements.txt (line 2)) (2022.12.7)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r ./diffusers/examples/dreambooth/requirements.txt (line 5))\n",
      "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->-r ./diffusers/examples/dreambooth/requirements.txt (line 5)) (3.2.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft==0.7.0->-r ./diffusers/examples/dreambooth/requirements.txt (line 7)) (1.3.0)\n",
      "Downloading peft-0.7.0-py3-none-any.whl (168 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl (6.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.36.1-py3-none-any.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth-2.25.2-py2.py3-none-any.whl (184 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.2/184.2 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading grpcio-1.60.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading Markdown-3.5.1-py3-none-any.whl (102 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading wcwidth-0.2.12-py2.py3-none-any.whl (34 kB)\n",
      "Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: wcwidth, werkzeug, tensorboard-data-server, pyasn1, protobuf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, markdown, grpcio, ftfy, cachetools, absl-py, rsa, requests-oauthlib, pyasn1-modules, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tokenizers, nvidia-cusolver-cu12, google-auth, transformers, torch, google-auth-oauthlib, torchvision, tensorboard, accelerate, peft\n",
      "  Attempting uninstall: wcwidth\n",
      "    Found existing installation: wcwidth 0.2.9\n",
      "    Uninstalling wcwidth-0.2.9:\n",
      "      Successfully uninstalled wcwidth-0.2.9\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.0+cu118\n",
      "    Uninstalling torch-2.1.0+cu118:\n",
      "      Successfully uninstalled torch-2.1.0+cu118\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.16.0+cu118\n",
      "    Uninstalling torchvision-0.16.0+cu118:\n",
      "      Successfully uninstalled torchvision-0.16.0+cu118\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed absl-py-2.0.0 accelerate-0.25.0 cachetools-5.3.2 ftfy-6.1.3 google-auth-2.25.2 google-auth-oauthlib-1.2.0 grpcio-1.60.0 markdown-3.5.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 peft-0.7.0 protobuf-4.23.4 pyasn1-0.5.1 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.15.1 tensorboard-data-server-0.7.2 tokenizers-0.15.0 torch-2.1.2 torchvision-0.16.2 transformers-4.36.1 wcwidth-0.2.12 werkzeug-3.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/diffusers.git\n",
    "!pip install ./diffusers\n",
    "!pip install -U -r ./diffusers/examples/dreambooth/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fed3d23a-2f1f-4cab-ab22-52c92d0f9ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "425a0e25-186c-480b-9e42-5df219554910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28dae04feb374db3bbcc2ef9a29d0ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0833b48a-fbd8-4ce7-a7bb-25207881c547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name, memory.total [MiB], memory.free [MiB]\n",
      "NVIDIA GeForce RTX 4090, 24564 MiB, 23710 MiB\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b8aa7ee-c589-44ad-a993-0012ea7765d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "import plot_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "859bdb00-2dab-4407-8a45-22cb433cb12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration already exists at /root/.cache/huggingface/accelerate/default_config.yaml, will not override. Run `accelerate config` manually or pass a different `save_location`.\n",
      "name, memory.total [MiB], memory.free [MiB]\n",
      "NVIDIA GeForce RTX 4090, 24564 MiB, 20218 MiB\n",
      "600\n",
      "12/15/2023 14:20:30 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'thresholding', 'variance_type', 'dynamic_thresholding_ratio', 'sample_max_value', 'clip_sample_range'} was not found in config. Values will be initialized to default values.\n",
      "12/15/2023 14:20:31 - INFO - __main__ - ***** Running training *****\n",
      "12/15/2023 14:20:31 - INFO - __main__ -   Num examples = 7\n",
      "12/15/2023 14:20:31 - INFO - __main__ -   Num batches each epoch = 7\n",
      "12/15/2023 14:20:31 - INFO - __main__ -   Num Epochs = 86\n",
      "12/15/2023 14:20:31 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "12/15/2023 14:20:31 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "12/15/2023 14:20:31 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "12/15/2023 14:20:31 - INFO - __main__ -   Total optimization steps = 600\n",
      "Steps:  83%|██████████  | 500/600 [01:39<00:19,  5.09it/s, loss=0.0725, lr=5e-6]12/15/2023 14:22:10 - INFO - accelerate.accelerator - Saving current state to ./dreambooth/linkedin_run12/checkpoint-500\n",
      "Configuration saved in ./dreambooth/linkedin_run12/checkpoint-500/unet/config.json\n",
      "Model weights saved in ./dreambooth/linkedin_run12/checkpoint-500/unet/diffusion_pytorch_model.safetensors\n",
      "12/15/2023 14:22:17 - INFO - accelerate.checkpointing - Optimizer state saved in dreambooth/linkedin_run12/checkpoint-500/optimizer.bin\n",
      "12/15/2023 14:22:17 - INFO - accelerate.checkpointing - Scheduler state saved in dreambooth/linkedin_run12/checkpoint-500/scheduler.bin\n",
      "12/15/2023 14:22:17 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in dreambooth/linkedin_run12/checkpoint-500/sampler.bin\n",
      "12/15/2023 14:22:17 - INFO - accelerate.checkpointing - Gradient scaler state saved in dreambooth/linkedin_run12/checkpoint-500/scaler.pt\n",
      "12/15/2023 14:22:17 - INFO - accelerate.checkpointing - Random states saved in dreambooth/linkedin_run12/checkpoint-500/random_states_0.pkl\n",
      "12/15/2023 14:22:17 - INFO - __main__ - Saved state to ./dreambooth/linkedin_run12/checkpoint-500\n",
      "Steps: 100%|███████████| 600/600 [02:05<00:00,  5.10it/s, loss=0.00595, lr=5e-6]\n",
      "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of ./dreambooth/black_background_run34.\n",
      "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of ./dreambooth/black_background_run34.\n",
      "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of ./dreambooth/black_background_run34.\n",
      "\n",
      "Loading pipeline components...:  57%|███████▍     | 4/7 [00:00<00:00, 21.46it/s]\u001b[ALoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of ./dreambooth/black_background_run34.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of ./dreambooth/black_background_run34.\n",
      "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 34.50it/s]\n",
      "Configuration saved in ./dreambooth/linkedin_run12/vae/config.json\n",
      "Model weights saved in ./dreambooth/linkedin_run12/vae/diffusion_pytorch_model.safetensors\n",
      "Configuration saved in ./dreambooth/linkedin_run12/unet/config.json\n",
      "Model weights saved in ./dreambooth/linkedin_run12/unet/diffusion_pytorch_model.safetensors\n",
      "Configuration saved in ./dreambooth/linkedin_run12/scheduler/scheduler_config.json\n",
      "Configuration saved in ./dreambooth/linkedin_run12/model_index.json\n",
      "Steps: 100%|███████████| 600/600 [02:12<00:00,  4.53it/s, loss=0.00595, lr=5e-6]\n",
      "Configuration already exists at /root/.cache/huggingface/accelerate/default_config.yaml, will not override. Run `accelerate config` manually or pass a different `save_location`.\n",
      "name, memory.total [MiB], memory.free [MiB]\n",
      "NVIDIA GeForce RTX 4090, 24564 MiB, 20218 MiB\n",
      "650\n",
      "12/15/2023 14:22:48 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'sample_max_value', 'variance_type', 'clip_sample_range', 'dynamic_thresholding_ratio', 'thresholding'} was not found in config. Values will be initialized to default values.\n",
      "12/15/2023 14:22:49 - INFO - __main__ - ***** Running training *****\n",
      "12/15/2023 14:22:49 - INFO - __main__ -   Num examples = 7\n",
      "12/15/2023 14:22:49 - INFO - __main__ -   Num batches each epoch = 7\n",
      "12/15/2023 14:22:49 - INFO - __main__ -   Num Epochs = 93\n",
      "12/15/2023 14:22:49 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "12/15/2023 14:22:49 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "12/15/2023 14:22:49 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "12/15/2023 14:22:49 - INFO - __main__ -   Total optimization steps = 650\n",
      "Steps:  77%|█████████▏  | 500/650 [01:38<00:29,  5.10it/s, loss=0.0515, lr=5e-6]12/15/2023 14:24:28 - INFO - accelerate.accelerator - Saving current state to ./dreambooth/linkedin_run13/checkpoint-500\n",
      "Configuration saved in ./dreambooth/linkedin_run13/checkpoint-500/unet/config.json\n",
      "Model weights saved in ./dreambooth/linkedin_run13/checkpoint-500/unet/diffusion_pytorch_model.safetensors\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 619, in save\n",
      "    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 853, in _save\n",
      "    zip_file.write_record(name, storage.data_ptr(), num_bytes)\n",
      "RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/932: file write failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/Machine_Learning_Institute/week8_fine_tune_stable_diffusion/dreambooth/./diffusers/examples/dreambooth/train_dreambooth.py\", line 1428, in <module>\n",
      "    main(args)\n",
      "  File \"/workspace/Machine_Learning_Institute/week8_fine_tune_stable_diffusion/dreambooth/./diffusers/examples/dreambooth/train_dreambooth.py\", line 1346, in main\n",
      "    accelerator.save_state(save_path)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 2708, in save_state\n",
      "    save_location = save_accelerator_state(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/checkpointing.py\", line 106, in save_accelerator_state\n",
      "    save(state, output_optimizer_file, save_on_each_node=save_on_each_node, safe_serialization=False)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/other.py\", line 181, in save\n",
      "    save_func(obj, f)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 618, in save\n",
      "    with _open_zipfile_writer(f) as opened_zipfile:\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 466, in __exit__\n",
      "    self.file_like.write_end_of_file()\n",
      "RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 6650969536 vs 6650969432\n",
      "Steps:  77%|█████████▏  | 500/650 [01:51<00:33,  4.50it/s, loss=0.0515, lr=5e-6]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py\", line 47, in main\n",
      "    args.func(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 1017, in launch_command\n",
      "    simple_launcher(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 637, in simple_launcher\n",
      "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
      "subprocess.CalledProcessError: Command '['/usr/bin/python', './diffusers/examples/dreambooth/train_dreambooth.py', '--pretrained_model_name_or_path=./dreambooth/black_background_run34', '--instance_data_dir=./linkedin_images', '--output_dir=./dreambooth/linkedin_run13', '--instance_prompt=lkdn_style', '--resolution=512', '--train_batch_size=1', '--gradient_accumulation_steps=1', '--learning_rate=5e-6', '--lr_scheduler=constant', '--lr_warmup_steps=0', '--max_train_steps=650']' returned non-zero exit status 1.\n",
      "Configuration already exists at /root/.cache/huggingface/accelerate/default_config.yaml, will not override. Run `accelerate config` manually or pass a different `save_location`.\n",
      "name, memory.total [MiB], memory.free [MiB]\n",
      "NVIDIA GeForce RTX 4090, 24564 MiB, 20218 MiB\n",
      "700\n",
      "12/15/2023 14:24:44 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'dynamic_thresholding_ratio', 'thresholding', 'clip_sample_range', 'variance_type', 'sample_max_value'} was not found in config. Values will be initialized to default values.\n",
      "12/15/2023 14:24:45 - INFO - __main__ - ***** Running training *****\n",
      "12/15/2023 14:24:45 - INFO - __main__ -   Num examples = 7\n",
      "12/15/2023 14:24:45 - INFO - __main__ -   Num batches each epoch = 7\n",
      "12/15/2023 14:24:45 - INFO - __main__ -   Num Epochs = 100\n",
      "12/15/2023 14:24:45 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "12/15/2023 14:24:45 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "12/15/2023 14:24:45 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "12/15/2023 14:24:45 - INFO - __main__ -   Total optimization steps = 700\n",
      "Steps:  71%|█████████▎   | 500/700 [01:38<00:39,  5.09it/s, loss=0.068, lr=5e-6]12/15/2023 14:26:24 - INFO - accelerate.accelerator - Saving current state to ./dreambooth/linkedin_run14/checkpoint-500\n",
      "Configuration saved in ./dreambooth/linkedin_run14/checkpoint-500/unet/config.json\n",
      "Model weights saved in ./dreambooth/linkedin_run14/checkpoint-500/unet/diffusion_pytorch_model.safetensors\n",
      "12/15/2023 14:26:33 - INFO - accelerate.checkpointing - Optimizer state saved in dreambooth/linkedin_run14/checkpoint-500/optimizer.bin\n",
      "12/15/2023 14:26:33 - INFO - accelerate.checkpointing - Scheduler state saved in dreambooth/linkedin_run14/checkpoint-500/scheduler.bin\n",
      "12/15/2023 14:26:33 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in dreambooth/linkedin_run14/checkpoint-500/sampler.bin\n",
      "12/15/2023 14:26:33 - INFO - accelerate.checkpointing - Gradient scaler state saved in dreambooth/linkedin_run14/checkpoint-500/scaler.pt\n",
      "12/15/2023 14:26:33 - INFO - accelerate.checkpointing - Random states saved in dreambooth/linkedin_run14/checkpoint-500/random_states_0.pkl\n",
      "12/15/2023 14:26:33 - INFO - __main__ - Saved state to ./dreambooth/linkedin_run14/checkpoint-500\n",
      "Steps: 100%|█████████████| 700/700 [02:26<00:00,  5.12it/s, loss=0.317, lr=5e-6]\n",
      "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of ./dreambooth/black_background_run34.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of ./dreambooth/black_background_run34.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of ./dreambooth/black_background_run34.\n",
      "\n",
      "Loading pipeline components...:  43%|█████▌       | 3/7 [00:00<00:00, 27.12it/s]\u001b[ALoaded scheduler as PNDMScheduler from `scheduler` subfolder of ./dreambooth/black_background_run34.\n",
      "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of ./dreambooth/black_background_run34.\n",
      "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 35.66it/s]\n",
      "Configuration saved in ./dreambooth/linkedin_run14/vae/config.json\n",
      "Model weights saved in ./dreambooth/linkedin_run14/vae/diffusion_pytorch_model.safetensors\n",
      "Configuration saved in ./dreambooth/linkedin_run14/unet/config.json\n",
      "Model weights saved in ./dreambooth/linkedin_run14/unet/diffusion_pytorch_model.safetensors\n",
      "Configuration saved in ./dreambooth/linkedin_run14/scheduler/scheduler_config.json\n",
      "Configuration saved in ./dreambooth/linkedin_run14/model_index.json\n",
      "Steps: 100%|█████████████| 700/700 [02:34<00:00,  4.53it/s, loss=0.317, lr=5e-6]\n",
      "Configuration already exists at /root/.cache/huggingface/accelerate/default_config.yaml, will not override. Run `accelerate config` manually or pass a different `save_location`.\n",
      "name, memory.total [MiB], memory.free [MiB]\n",
      "NVIDIA GeForce RTX 4090, 24564 MiB, 20218 MiB\n",
      "750\n",
      "12/15/2023 14:27:24 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'variance_type', 'clip_sample_range', 'sample_max_value', 'dynamic_thresholding_ratio', 'thresholding'} was not found in config. Values will be initialized to default values.\n",
      "12/15/2023 14:27:25 - INFO - __main__ - ***** Running training *****\n",
      "12/15/2023 14:27:25 - INFO - __main__ -   Num examples = 7\n",
      "12/15/2023 14:27:25 - INFO - __main__ -   Num batches each epoch = 7\n",
      "12/15/2023 14:27:25 - INFO - __main__ -   Num Epochs = 108\n",
      "12/15/2023 14:27:25 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "12/15/2023 14:27:25 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "12/15/2023 14:27:25 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "12/15/2023 14:27:25 - INFO - __main__ -   Total optimization steps = 750\n",
      "Steps:  67%|████████▋    | 500/750 [01:38<00:49,  5.08it/s, loss=0.095, lr=5e-6]12/15/2023 14:29:04 - INFO - accelerate.accelerator - Saving current state to ./dreambooth/linkedin_run15/checkpoint-500\n",
      "Configuration saved in ./dreambooth/linkedin_run15/checkpoint-500/unet/config.json\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/Machine_Learning_Institute/week8_fine_tune_stable_diffusion/dreambooth/./diffusers/examples/dreambooth/train_dreambooth.py\", line 1428, in <module>\n",
      "    main(args)\n",
      "  File \"/workspace/Machine_Learning_Institute/week8_fine_tune_stable_diffusion/dreambooth/./diffusers/examples/dreambooth/train_dreambooth.py\", line 1346, in main\n",
      "    accelerator.save_state(save_path)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 2706, in save_state\n",
      "    hook(self._models, weights, output_dir)\n",
      "  File \"/workspace/Machine_Learning_Institute/week8_fine_tune_stable_diffusion/dreambooth/./diffusers/examples/dreambooth/train_dreambooth.py\", line 939, in save_model_hook\n",
      "    model.save_pretrained(os.path.join(output_dir, sub_dir))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/diffusers/models/modeling_utils.py\", line 519, in save_pretrained\n",
      "    safetensors.torch.save_file(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/safetensors/torch.py\", line 281, in save_file\n",
      "    serialize_file(_flatten(tensors), filename, metadata=metadata)\n",
      "safetensors_rust.SafetensorError: Error while serializing: IoError(Os { code: 28, kind: StorageFull, message: \"No space left on device\" })\n",
      "Steps:  67%|████████▋    | 500/750 [01:40<00:50,  4.97it/s, loss=0.095, lr=5e-6]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py\", line 47, in main\n",
      "    args.func(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 1017, in launch_command\n",
      "    simple_launcher(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 637, in simple_launcher\n",
      "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
      "subprocess.CalledProcessError: Command '['/usr/bin/python', './diffusers/examples/dreambooth/train_dreambooth.py', '--pretrained_model_name_or_path=./dreambooth/black_background_run34', '--instance_data_dir=./linkedin_images', '--output_dir=./dreambooth/linkedin_run15', '--instance_prompt=lkdn_style', '--resolution=512', '--train_batch_size=1', '--gradient_accumulation_steps=1', '--learning_rate=5e-6', '--lr_scheduler=constant', '--lr_warmup_steps=0', '--max_train_steps=750']' returned non-zero exit status 1.\n",
      "Configuration already exists at /root/.cache/huggingface/accelerate/default_config.yaml, will not override. Run `accelerate config` manually or pass a different `save_location`.\n",
      "name, memory.total [MiB], memory.free [MiB]\n",
      "NVIDIA GeForce RTX 4090, 24564 MiB, 20218 MiB\n",
      "800\n",
      "12/15/2023 14:29:10 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'variance_type', 'clip_sample_range', 'sample_max_value', 'dynamic_thresholding_ratio', 'thresholding'} was not found in config. Values will be initialized to default values.\n",
      "12/15/2023 14:29:11 - INFO - __main__ - ***** Running training *****\n",
      "12/15/2023 14:29:11 - INFO - __main__ -   Num examples = 7\n",
      "12/15/2023 14:29:11 - INFO - __main__ -   Num batches each epoch = 7\n",
      "12/15/2023 14:29:11 - INFO - __main__ -   Num Epochs = 115\n",
      "12/15/2023 14:29:11 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "12/15/2023 14:29:11 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "12/15/2023 14:29:11 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "12/15/2023 14:29:11 - INFO - __main__ -   Total optimization steps = 800\n",
      "Steps:  62%|████████▏    | 500/800 [01:38<00:58,  5.09it/s, loss=0.351, lr=5e-6]12/15/2023 14:30:50 - INFO - accelerate.accelerator - Saving current state to ./dreambooth/linkedin_run16/checkpoint-500\n",
      "Configuration saved in ./dreambooth/linkedin_run16/checkpoint-500/unet/config.json\n",
      "Model weights saved in ./dreambooth/linkedin_run16/checkpoint-500/unet/diffusion_pytorch_model.safetensors\n",
      "12/15/2023 14:30:59 - INFO - accelerate.checkpointing - Optimizer state saved in dreambooth/linkedin_run16/checkpoint-500/optimizer.bin\n",
      "12/15/2023 14:30:59 - INFO - accelerate.checkpointing - Scheduler state saved in dreambooth/linkedin_run16/checkpoint-500/scheduler.bin\n",
      "12/15/2023 14:30:59 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in dreambooth/linkedin_run16/checkpoint-500/sampler.bin\n",
      "12/15/2023 14:30:59 - INFO - accelerate.checkpointing - Gradient scaler state saved in dreambooth/linkedin_run16/checkpoint-500/scaler.pt\n",
      "12/15/2023 14:30:59 - INFO - accelerate.checkpointing - Random states saved in dreambooth/linkedin_run16/checkpoint-500/random_states_0.pkl\n",
      "12/15/2023 14:30:59 - INFO - __main__ - Saved state to ./dreambooth/linkedin_run16/checkpoint-500\n",
      "Steps: 100%|████████████| 800/800 [02:46<00:00,  4.99it/s, loss=0.0243, lr=5e-6]\n",
      "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of ./dreambooth/black_background_run34.\n",
      "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of ./dreambooth/black_background_run34.\n",
      "\n",
      "Loading pipeline components...:  29%|███▋         | 2/7 [00:00<00:00, 10.76it/s]\u001b[ALoaded vae as AutoencoderKL from `vae` subfolder of ./dreambooth/black_background_run34.\n",
      "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of ./dreambooth/black_background_run34.\n",
      "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of ./dreambooth/black_background_run34.\n",
      "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 34.30it/s]\n",
      "Configuration saved in ./dreambooth/linkedin_run16/vae/config.json\n",
      "Model weights saved in ./dreambooth/linkedin_run16/vae/diffusion_pytorch_model.safetensors\n",
      "Configuration saved in ./dreambooth/linkedin_run16/unet/config.json\n",
      "Model weights saved in ./dreambooth/linkedin_run16/unet/diffusion_pytorch_model.safetensors\n",
      "Configuration saved in ./dreambooth/linkedin_run16/scheduler/scheduler_config.json\n",
      "Configuration saved in ./dreambooth/linkedin_run16/model_index.json\n",
      "Steps: 100%|████████████| 800/800 [02:54<00:00,  4.57it/s, loss=0.0243, lr=5e-6]\n",
      "Configuration already exists at /root/.cache/huggingface/accelerate/default_config.yaml, will not override. Run `accelerate config` manually or pass a different `save_location`.\n",
      "name, memory.total [MiB], memory.free [MiB]\n",
      "NVIDIA GeForce RTX 4090, 24564 MiB, 20218 MiB\n",
      "850\n",
      "12/15/2023 14:32:10 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'dynamic_thresholding_ratio', 'thresholding', 'sample_max_value', 'variance_type', 'clip_sample_range'} was not found in config. Values will be initialized to default values.\n",
      "12/15/2023 14:32:11 - INFO - __main__ - ***** Running training *****\n",
      "12/15/2023 14:32:11 - INFO - __main__ -   Num examples = 7\n",
      "12/15/2023 14:32:11 - INFO - __main__ -   Num batches each epoch = 7\n",
      "12/15/2023 14:32:11 - INFO - __main__ -   Num Epochs = 122\n",
      "12/15/2023 14:32:11 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "12/15/2023 14:32:11 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "12/15/2023 14:32:11 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "12/15/2023 14:32:11 - INFO - __main__ -   Total optimization steps = 850\n",
      "Steps:  59%|██████▍    | 500/850 [01:38<01:08,  5.09it/s, loss=0.00519, lr=5e-6]12/15/2023 14:33:50 - INFO - accelerate.accelerator - Saving current state to ./dreambooth/linkedin_run17/checkpoint-500\n",
      "Configuration saved in ./dreambooth/linkedin_run17/checkpoint-500/unet/config.json\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/Machine_Learning_Institute/week8_fine_tune_stable_diffusion/dreambooth/./diffusers/examples/dreambooth/train_dreambooth.py\", line 1428, in <module>\n",
      "    main(args)\n",
      "  File \"/workspace/Machine_Learning_Institute/week8_fine_tune_stable_diffusion/dreambooth/./diffusers/examples/dreambooth/train_dreambooth.py\", line 1346, in main\n",
      "    accelerator.save_state(save_path)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 2706, in save_state\n",
      "    hook(self._models, weights, output_dir)\n",
      "  File \"/workspace/Machine_Learning_Institute/week8_fine_tune_stable_diffusion/dreambooth/./diffusers/examples/dreambooth/train_dreambooth.py\", line 939, in save_model_hook\n",
      "    model.save_pretrained(os.path.join(output_dir, sub_dir))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/diffusers/models/modeling_utils.py\", line 519, in save_pretrained\n",
      "    safetensors.torch.save_file(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/safetensors/torch.py\", line 281, in save_file\n",
      "    serialize_file(_flatten(tensors), filename, metadata=metadata)\n",
      "safetensors_rust.SafetensorError: Error while serializing: IoError(Os { code: 28, kind: StorageFull, message: \"No space left on device\" })\n",
      "Steps:  59%|██████▍    | 500/850 [01:40<01:10,  4.97it/s, loss=0.00519, lr=5e-6]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py\", line 47, in main\n",
      "    args.func(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 1017, in launch_command\n",
      "    simple_launcher(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 637, in simple_launcher\n",
      "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
      "subprocess.CalledProcessError: Command '['/usr/bin/python', './diffusers/examples/dreambooth/train_dreambooth.py', '--pretrained_model_name_or_path=./dreambooth/black_background_run34', '--instance_data_dir=./linkedin_images', '--output_dir=./dreambooth/linkedin_run17', '--instance_prompt=lkdn_style', '--resolution=512', '--train_batch_size=1', '--gradient_accumulation_steps=1', '--learning_rate=5e-6', '--lr_scheduler=constant', '--lr_warmup_steps=0', '--max_train_steps=850']' returned non-zero exit status 1.\n",
      "Configuration already exists at /root/.cache/huggingface/accelerate/default_config.yaml, will not override. Run `accelerate config` manually or pass a different `save_location`.\n",
      "name, memory.total [MiB], memory.free [MiB]\n",
      "NVIDIA GeForce RTX 4090, 24564 MiB, 20218 MiB\n",
      "900\n",
      "12/15/2023 14:33:56 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'clip_sample_range', 'thresholding', 'sample_max_value', 'variance_type', 'dynamic_thresholding_ratio'} was not found in config. Values will be initialized to default values.\n",
      "12/15/2023 14:33:57 - INFO - __main__ - ***** Running training *****\n",
      "12/15/2023 14:33:57 - INFO - __main__ -   Num examples = 7\n",
      "12/15/2023 14:33:57 - INFO - __main__ -   Num batches each epoch = 7\n",
      "12/15/2023 14:33:57 - INFO - __main__ -   Num Epochs = 129\n",
      "12/15/2023 14:33:57 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "12/15/2023 14:33:57 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "12/15/2023 14:33:57 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "12/15/2023 14:33:57 - INFO - __main__ -   Total optimization steps = 900\n",
      "Steps:  56%|██████▋     | 500/900 [01:38<01:18,  5.08it/s, loss=0.0383, lr=5e-6]12/15/2023 14:35:36 - INFO - accelerate.accelerator - Saving current state to ./dreambooth/linkedin_run18/checkpoint-500\n",
      "Configuration saved in ./dreambooth/linkedin_run18/checkpoint-500/unet/config.json\n",
      "Model weights saved in ./dreambooth/linkedin_run18/checkpoint-500/unet/diffusion_pytorch_model.safetensors\n",
      "12/15/2023 14:35:44 - INFO - accelerate.checkpointing - Optimizer state saved in dreambooth/linkedin_run18/checkpoint-500/optimizer.bin\n",
      "12/15/2023 14:35:44 - INFO - accelerate.checkpointing - Scheduler state saved in dreambooth/linkedin_run18/checkpoint-500/scheduler.bin\n",
      "12/15/2023 14:35:44 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in dreambooth/linkedin_run18/checkpoint-500/sampler.bin\n",
      "12/15/2023 14:35:44 - INFO - accelerate.checkpointing - Gradient scaler state saved in dreambooth/linkedin_run18/checkpoint-500/scaler.pt\n",
      "12/15/2023 14:35:45 - INFO - accelerate.checkpointing - Random states saved in dreambooth/linkedin_run18/checkpoint-500/random_states_0.pkl\n",
      "12/15/2023 14:35:45 - INFO - __main__ - Saved state to ./dreambooth/linkedin_run18/checkpoint-500\n",
      "Steps: 100%|████████████| 900/900 [03:06<00:00,  5.10it/s, loss=0.0372, lr=5e-6]\n",
      "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of ./dreambooth/black_background_run34.\n",
      "\n",
      "Loading pipeline components...:  14%|█▊           | 1/7 [00:00<00:00,  6.08it/s]\u001b[ALoaded scheduler as PNDMScheduler from `scheduler` subfolder of ./dreambooth/black_background_run34.\n",
      "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of ./dreambooth/black_background_run34.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of ./dreambooth/black_background_run34.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of ./dreambooth/black_background_run34.\n",
      "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 32.60it/s]\n",
      "Configuration saved in ./dreambooth/linkedin_run18/vae/config.json\n",
      "Model weights saved in ./dreambooth/linkedin_run18/vae/diffusion_pytorch_model.safetensors\n",
      "Configuration saved in ./dreambooth/linkedin_run18/unet/config.json\n",
      "Model weights saved in ./dreambooth/linkedin_run18/unet/diffusion_pytorch_model.safetensors\n",
      "Configuration saved in ./dreambooth/linkedin_run18/scheduler/scheduler_config.json\n",
      "Configuration saved in ./dreambooth/linkedin_run18/model_index.json\n",
      "Steps: 100%|████████████| 900/900 [03:13<00:00,  4.65it/s, loss=0.0372, lr=5e-6]\n",
      "Configuration already exists at /root/.cache/huggingface/accelerate/default_config.yaml, will not override. Run `accelerate config` manually or pass a different `save_location`.\n",
      "name, memory.total [MiB], memory.free [MiB]\n",
      "NVIDIA GeForce RTX 4090, 24564 MiB, 20218 MiB\n",
      "950\n",
      "12/15/2023 14:37:15 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'thresholding', 'dynamic_thresholding_ratio', 'clip_sample_range', 'sample_max_value', 'variance_type'} was not found in config. Values will be initialized to default values.\n",
      "12/15/2023 14:37:16 - INFO - __main__ - ***** Running training *****\n",
      "12/15/2023 14:37:16 - INFO - __main__ -   Num examples = 7\n",
      "12/15/2023 14:37:16 - INFO - __main__ -   Num batches each epoch = 7\n",
      "12/15/2023 14:37:16 - INFO - __main__ -   Num Epochs = 136\n",
      "12/15/2023 14:37:16 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "12/15/2023 14:37:16 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "12/15/2023 14:37:16 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "12/15/2023 14:37:16 - INFO - __main__ -   Total optimization steps = 950\n",
      "Steps:  53%|██████▎     | 500/950 [01:38<01:28,  5.08it/s, loss=0.0221, lr=5e-6]12/15/2023 14:38:54 - INFO - accelerate.accelerator - Saving current state to ./dreambooth/linkedin_run19/checkpoint-500\n",
      "Configuration saved in ./dreambooth/linkedin_run19/checkpoint-500/unet/config.json\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/Machine_Learning_Institute/week8_fine_tune_stable_diffusion/dreambooth/./diffusers/examples/dreambooth/train_dreambooth.py\", line 1428, in <module>\n",
      "    main(args)\n",
      "  File \"/workspace/Machine_Learning_Institute/week8_fine_tune_stable_diffusion/dreambooth/./diffusers/examples/dreambooth/train_dreambooth.py\", line 1346, in main\n",
      "    accelerator.save_state(save_path)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 2706, in save_state\n",
      "    hook(self._models, weights, output_dir)\n",
      "  File \"/workspace/Machine_Learning_Institute/week8_fine_tune_stable_diffusion/dreambooth/./diffusers/examples/dreambooth/train_dreambooth.py\", line 939, in save_model_hook\n",
      "    model.save_pretrained(os.path.join(output_dir, sub_dir))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/diffusers/models/modeling_utils.py\", line 519, in save_pretrained\n",
      "    safetensors.torch.save_file(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/safetensors/torch.py\", line 281, in save_file\n",
      "    serialize_file(_flatten(tensors), filename, metadata=metadata)\n",
      "safetensors_rust.SafetensorError: Error while serializing: IoError(Os { code: 28, kind: StorageFull, message: \"No space left on device\" })\n",
      "Steps:  53%|██████▎     | 500/950 [01:40<01:30,  4.98it/s, loss=0.0221, lr=5e-6]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py\", line 47, in main\n",
      "    args.func(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 1017, in launch_command\n",
      "    simple_launcher(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 637, in simple_launcher\n",
      "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
      "subprocess.CalledProcessError: Command '['/usr/bin/python', './diffusers/examples/dreambooth/train_dreambooth.py', '--pretrained_model_name_or_path=./dreambooth/black_background_run34', '--instance_data_dir=./linkedin_images', '--output_dir=./dreambooth/linkedin_run19', '--instance_prompt=lkdn_style', '--resolution=512', '--train_batch_size=1', '--gradient_accumulation_steps=1', '--learning_rate=5e-6', '--lr_scheduler=constant', '--lr_warmup_steps=0', '--max_train_steps=950']' returned non-zero exit status 1.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "for i in range(12,20,1):\n",
    "    import torch\n",
    "    import gc\n",
    "    import os\n",
    "    import plot_grid\n",
    "    !accelerate config default --mixed_precision fp16\n",
    "    \n",
    "    !nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv\n",
    "    os.environ['MODEL_NAME'] = './dreambooth/black_background_run34'\n",
    "    os.environ['INSTANCE_DIR'] = './linkedin_images'\n",
    "    os.environ['OUTPUT_DIR'] = f'./dreambooth/linkedin_run{i}'\n",
    "    os.environ['TRAIN_STEPS'] = str(i*50)\n",
    "    print(i*50)\n",
    "    !accelerate launch ./diffusers/examples/dreambooth/train_dreambooth.py \\\n",
    "      --pretrained_model_name_or_path=$MODEL_NAME  \\\n",
    "      --instance_data_dir=$INSTANCE_DIR \\\n",
    "      --output_dir=$OUTPUT_DIR \\\n",
    "      --instance_prompt=\"lkdn_style\" \\\n",
    "      --resolution=512 \\\n",
    "      --train_batch_size=1 \\\n",
    "      --gradient_accumulation_steps=1 \\\n",
    "      --learning_rate=5e-6 \\\n",
    "      --lr_scheduler=\"constant\" \\\n",
    "      --lr_warmup_steps=0 \\\n",
    "      --max_train_steps=$TRAIN_STEPS\n",
    "    %reset -f\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2cd11c-72b4-4933-b99b-f9fe211db0df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
