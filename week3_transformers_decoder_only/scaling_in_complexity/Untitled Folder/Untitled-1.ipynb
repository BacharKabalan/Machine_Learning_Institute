{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "vocab_size = 16000\n",
    "input_embedding_dimensions = 512\n",
    "max_sequence_length = 1194\n",
    "ff_dimension = 2048\n",
    "num_heads=2\n",
    "num_layers = 3\n",
    "device = 'cpu'\n",
    "\n",
    "\n",
    "class positionalEmbeddings(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_embedding_dimensions = input_embedding_dimensions\n",
    "        self.input_embeddings = nn.Embedding(vocab_size, input_embedding_dimensions)\n",
    "        self.positional_encoding = nn.Embedding(max_sequence_length, input_embedding_dimensions)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        input_embeddings = self.input_embeddings(inputs)\n",
    "        positional_embeddings = self.positional_encoding(torch.arange(inputs.size(1)).to(device))\n",
    "        positional_embeddings = input_embeddings + positional_embeddings\n",
    "        \n",
    "        return positional_embeddings\n",
    "\n",
    "\n",
    "class attention(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #MHA\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = input_embedding_dimensions // self.num_heads\n",
    "        self.mha_first_linear_layer = nn.Linear(input_embedding_dimensions,3*input_embedding_dimensions)\n",
    "        self.register_buffer('mask', torch.tril(torch.ones(max_sequence_length,max_sequence_length).view(1, 1,max_sequence_length,max_sequence_length)))\n",
    "        self.mha_final_linear_layer = nn.Linear(input_embedding_dimensions,input_embedding_dimensions)\n",
    "    \n",
    "    def forward(self, positional_embeddings):\n",
    "\n",
    "        heads_output = []\n",
    "        heads_att_weights = []\n",
    "        batch_size = positional_embeddings.size(0)\n",
    "        #input splitting (Q,K,V)\n",
    "        \n",
    "        q,k,v = self.mha_first_linear_layer(positional_embeddings).split(input_embedding_dimensions,dim=-1)\n",
    "        q = self.split_heads(q,batch_size)\n",
    "        k = self.split_heads(k,batch_size)\n",
    "        v = self.split_heads(v,batch_size)\n",
    "        #MHA\n",
    "        score_matrix = q @ k.permute(0,1,3,2)\n",
    "        score_matrix = score_matrix/(self.head_dim ** 0.5)\n",
    "        mask = self.mask == 0 #torch.tril(torch.ones(batch_size,1,score_matrix.size(2), score_matrix.size(3))) == 0\n",
    "        mask = mask[:,:,:score_matrix.size(2),:score_matrix.size(2)]\n",
    "        score_matrix = score_matrix.masked_fill(mask, float('-inf'))\n",
    "        att_weights = torch.softmax(score_matrix,dim=-1)\n",
    "        heads_att_weights.append(att_weights)\n",
    "        output = att_weights @ v\n",
    "        output = output.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.head_dim * self.num_heads)\n",
    "\n",
    "        heads_output.append(output)\n",
    "        heads_output = torch.cat(heads_output, dim=-1)\n",
    "\n",
    "        final_mha_output = self.mha_final_linear_layer(heads_output)\n",
    "        return final_mha_output\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "\n",
    "class feedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ff_first_linear = nn.Linear(input_embedding_dimensions,ff_dimension)\n",
    "        self.ff_relu = nn.ReLU()\n",
    "        self.ff_second_linear = nn.Linear(ff_dimension,input_embedding_dimensions)\n",
    "\n",
    "    def forward(self, mha_norm_output):\n",
    "        ff_first_linear_output = self.ff_first_linear(mha_norm_output)\n",
    "        ff_relu_output = self.ff_relu(ff_first_linear_output)\n",
    "        ff_output = self.ff_second_linear(ff_relu_output)\n",
    "        return ff_output\n",
    "\n",
    "\n",
    "class transformer_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attention = attention()\n",
    "        self.mha_norm_layer = nn.LayerNorm(input_embedding_dimensions) # this was mha_add_norm.size(-1) and i replaced with input_embedding_dimensions\n",
    "        self.feedForward = feedForward()\n",
    "        self.ff_norm_layer = nn.LayerNorm(input_embedding_dimensions) # this was ff_add_norm.size(-1) and i replaced with input_embedding_dimensions\n",
    "    \n",
    "    def forward(self, positional_embeddings):\n",
    "        final_mha_output = self.attention(positional_embeddings)\n",
    "        mha_add_norm = final_mha_output + positional_embeddings\n",
    "        mha_norm_output = self.mha_norm_layer(mha_add_norm)\n",
    "        ff_output = self.feedForward(mha_norm_output)\n",
    "\n",
    "        ff_add_norm = ff_output + mha_norm_output\n",
    "        ff_norm_output = self.ff_norm_layer(ff_add_norm)\n",
    "        return ff_norm_output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.positional_embeddings = positionalEmbeddings()\n",
    "        self.transformer_layer = torch.nn.ModuleList([transformer_layer() for _ in range(num_layers)])\n",
    "        self.final_linear_layer = nn.Linear(input_embedding_dimensions,vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        positional_embeddings = self.positional_embeddings(inputs)\n",
    "        for transformer_layer in self.transformer_layer: positional_embeddings  = transformer_layer(positional_embeddings)\n",
    "        ff_norm_output = positional_embeddings\n",
    "        final_linear_layer_output = self.final_linear_layer(ff_norm_output)\n",
    "        final_transformer_output = final_linear_layer_output #torch.softmax(final_linear_layer_output, dim=-1)\n",
    "        return final_transformer_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model = GPT().to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.load('multi_head_with_pos_encod_weights_0_525000.pt',map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The boy went to She She She She She She She She She She She She She She She She She He was He He was He He was He He was He Joe Joe He Joe Joe Joe He Joe Joe He Joe Joe Joe He Joe He Joe Joe He Joe Joe Joe He Joe Joe He Joe He Joe He Joe He Joe Joe He Joe He Joe He Joe He Joe Joe He Joe He Joe He Joe He Joe He Joe Joe Joe Joe Joe Joe Joe Joe Joe Joe Joe Joe He Joe He Joe Billy He Joe He Joe He Joe He Joe He Joe He Joe He Joe He Joe Joe Joe Joe Joe Joe Joe Joe Joe Joe Joe Joe Joe Joe Joe Joe Joe Joe Joe Joe Joe Joe Joe Joe Joe Joe Joe Joe Joe Joe Joe Joe Joe Joe Joe Joe Joe Joe Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy Timmy\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Import Libraries\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Importing the Transformer architecture\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Write the inference function\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "\n",
    "vocab_size = 16000\n",
    "input_embedding_dimensions = 512\n",
    "max_sequence_length = 1194\n",
    "ff_dimension = 2048\n",
    "num_heads = 1\n",
    "\n",
    "# Instantiate the model\n",
    "# Step 3: Load the pre-trained weights\n",
    "inference_model.load_state_dict(g['model_state_dict'])\n",
    "\n",
    "inference_model = inference_model.to('cpu')\n",
    "# Set the model to evaluation mode (important for models with batch normalization and dropout)\n",
    "inference_model.eval()\n",
    "with torch.no_grad():\n",
    "    tk = spm.SentencePieceProcessor()\n",
    "    tk.load('tiny_piece.model')\n",
    "\n",
    "    tokenised_text = tk.EncodeAsIds('The boy went to')\n",
    "    iterable_text = tokenised_text\n",
    "\n",
    "    next_token_id = tk.PieceToId(\"[BOS]\")\n",
    "    break_count = 0\n",
    "\n",
    "    while (next_token_id != tk.PieceToId(\"[EOS]\")) & (break_count <= 300):\n",
    "        iterable_tensor = torch.tensor(iterable_text)\n",
    "        iterable_tensor = iterable_tensor.view(1,-1)\n",
    "\n",
    "        probability_matrix = inference_model(iterable_tensor)\n",
    "        #print(probability_matrix.size())\n",
    "        probability_vector = probability_matrix[0, -1, :]\n",
    "        #print(probability_vector.size())\n",
    "        next_token_id = (torch.argmax(probability_vector))\n",
    "        #print(next_token_id)\n",
    "        iterable_text = iterable_text + [next_token_id.item()]\n",
    "        #print(tk.decode(iterable_text))\n",
    "        break_count += 1\n",
    "    \n",
    "final_text = tk.decode(iterable_text)\n",
    "print(final_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
