{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "vocab_size = 16000\n",
    "input_embedding_dimensions = 512\n",
    "max_sequence_length = 1194\n",
    "ff_dimension = 2048\n",
    "num_heads=2\n",
    "num_layers = 3\n",
    "device = 'cuda:0'\n",
    "\n",
    "\n",
    "class positionalEmbeddings(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_embedding_dimensions = input_embedding_dimensions\n",
    "        self.input_embeddings = nn.Embedding(vocab_size, input_embedding_dimensions)\n",
    "        self.positional_encoding = nn.Embedding(max_sequence_length, input_embedding_dimensions)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        input_embeddings = self.input_embeddings(inputs)\n",
    "        positional_embeddings = self.positional_encoding(torch.arange(inputs.size(1)).to(device))\n",
    "        positional_embeddings = input_embeddings + positional_embeddings\n",
    "        \n",
    "        return positional_embeddings\n",
    "\n",
    "\n",
    "class attention(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #MHA\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = input_embedding_dimensions // self.num_heads\n",
    "        self.mha_first_linear_layer = nn.Linear(input_embedding_dimensions,3*input_embedding_dimensions)\n",
    "        self.register_buffer('mask', torch.tril(torch.ones(max_sequence_length,max_sequence_length).view(1, 1,max_sequence_length,max_sequence_length)))\n",
    "        self.mha_final_linear_layer = nn.Linear(input_embedding_dimensions,input_embedding_dimensions)\n",
    "    \n",
    "    def forward(self, positional_embeddings):\n",
    "\n",
    "        heads_output = []\n",
    "        heads_att_weights = []\n",
    "        batch_size = positional_embeddings.size(0)\n",
    "        #input splitting (Q,K,V)\n",
    "        \n",
    "        q,k,v = self.mha_first_linear_layer(positional_embeddings).split(input_embedding_dimensions,dim=-1)\n",
    "        q = self.split_heads(q,batch_size)\n",
    "        k = self.split_heads(k,batch_size)\n",
    "        v = self.split_heads(v,batch_size)\n",
    "        #MHA\n",
    "        score_matrix = q @ k.permute(0,1,3,2)\n",
    "        score_matrix = score_matrix/(self.head_dim ** 0.5)\n",
    "        mask = self.mask == 0 #torch.tril(torch.ones(batch_size,1,score_matrix.size(2), score_matrix.size(3))) == 0\n",
    "        mask = mask[:,:,:score_matrix.size(2),:score_matrix.size(2)]\n",
    "        score_matrix = score_matrix.masked_fill(mask, float('-inf'))\n",
    "        att_weights = torch.softmax(score_matrix,dim=-1)\n",
    "        heads_att_weights.append(att_weights)\n",
    "        output = att_weights @ v\n",
    "        output = output.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.head_dim * self.num_heads)\n",
    "\n",
    "        heads_output.append(output)\n",
    "        heads_output = torch.cat(heads_output, dim=-1)\n",
    "\n",
    "        final_mha_output = self.mha_final_linear_layer(heads_output)\n",
    "        return final_mha_output\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "\n",
    "class feedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ff_first_linear = nn.Linear(input_embedding_dimensions,ff_dimension)\n",
    "        self.ff_relu = nn.ReLU()\n",
    "        self.ff_second_linear = nn.Linear(ff_dimension,input_embedding_dimensions)\n",
    "\n",
    "    def forward(self, mha_norm_output):\n",
    "        ff_first_linear_output = self.ff_first_linear(mha_norm_output)\n",
    "        ff_relu_output = self.ff_relu(ff_first_linear_output)\n",
    "        ff_output = self.ff_second_linear(ff_relu_output)\n",
    "        return ff_output\n",
    "\n",
    "\n",
    "class transformer_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attention = attention()\n",
    "        self.mha_norm_layer = nn.LayerNorm(input_embedding_dimensions) # this was mha_add_norm.size(-1) and i replaced with input_embedding_dimensions\n",
    "        self.feedForward = feedForward()\n",
    "        self.ff_norm_layer = nn.LayerNorm(input_embedding_dimensions) # this was ff_add_norm.size(-1) and i replaced with input_embedding_dimensions\n",
    "    \n",
    "    def forward(self, positional_embeddings):\n",
    "        final_mha_output = self.attention(positional_embeddings)\n",
    "        mha_add_norm = final_mha_output + positional_embeddings\n",
    "        mha_norm_output = self.mha_norm_layer(mha_add_norm)\n",
    "        ff_output = self.feedForward(mha_norm_output)\n",
    "\n",
    "        ff_add_norm = ff_output + mha_norm_output\n",
    "        ff_norm_output = self.ff_norm_layer(ff_add_norm)\n",
    "        return ff_norm_output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.positional_embeddings = positionalEmbeddings()\n",
    "        self.transformer_layer = torch.nn.ModuleList([transformer_layer() for _ in range(num_layers)])\n",
    "        self.final_linear_layer = nn.Linear(input_embedding_dimensions,vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        positional_embeddings = self.positional_embeddings(inputs)\n",
    "        for transformer_layer in self.transformer_layer: positional_embeddings  = transformer_layer(positional_embeddings)\n",
    "        ff_norm_output = positional_embeddings\n",
    "        final_linear_layer_output = self.final_linear_layer(ff_norm_output)\n",
    "        final_transformer_output = final_linear_layer_output #torch.softmax(final_linear_layer_output, dim=-1)\n",
    "        return final_transformer_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model = GPT().to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['positional_embeddings.input_embeddings.weight', 'positional_embeddings.positional_encoding.weight', 'transformer_layer.0.attention.mask', 'transformer_layer.0.attention.mha_first_linear_layer.weight', 'transformer_layer.0.attention.mha_first_linear_layer.bias', 'transformer_layer.0.attention.mha_final_linear_layer.weight', 'transformer_layer.0.attention.mha_final_linear_layer.bias', 'transformer_layer.0.mha_norm_layer.weight', 'transformer_layer.0.mha_norm_layer.bias', 'transformer_layer.0.feedForward.ff_first_linear.weight', 'transformer_layer.0.feedForward.ff_first_linear.bias', 'transformer_layer.0.feedForward.ff_second_linear.weight', 'transformer_layer.0.feedForward.ff_second_linear.bias', 'transformer_layer.0.ff_norm_layer.weight', 'transformer_layer.0.ff_norm_layer.bias', 'transformer_layer.1.attention.mask', 'transformer_layer.1.attention.mha_first_linear_layer.weight', 'transformer_layer.1.attention.mha_first_linear_layer.bias', 'transformer_layer.1.attention.mha_final_linear_layer.weight', 'transformer_layer.1.attention.mha_final_linear_layer.bias', 'transformer_layer.1.mha_norm_layer.weight', 'transformer_layer.1.mha_norm_layer.bias', 'transformer_layer.1.feedForward.ff_first_linear.weight', 'transformer_layer.1.feedForward.ff_first_linear.bias', 'transformer_layer.1.feedForward.ff_second_linear.weight', 'transformer_layer.1.feedForward.ff_second_linear.bias', 'transformer_layer.1.ff_norm_layer.weight', 'transformer_layer.1.ff_norm_layer.bias', 'transformer_layer.2.attention.mask', 'transformer_layer.2.attention.mha_first_linear_layer.weight', 'transformer_layer.2.attention.mha_first_linear_layer.bias', 'transformer_layer.2.attention.mha_final_linear_layer.weight', 'transformer_layer.2.attention.mha_final_linear_layer.bias', 'transformer_layer.2.mha_norm_layer.weight', 'transformer_layer.2.mha_norm_layer.bias', 'transformer_layer.2.feedForward.ff_first_linear.weight', 'transformer_layer.2.feedForward.ff_first_linear.bias', 'transformer_layer.2.feedForward.ff_second_linear.weight', 'transformer_layer.2.feedForward.ff_second_linear.bias', 'transformer_layer.2.ff_norm_layer.weight', 'transformer_layer.2.ff_norm_layer.bias', 'final_linear_layer.weight', 'final_linear_layer.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved state dictionary\n",
    "saved_state_dict = torch.load('multi_head_with_pos_encod_weights_0_525000.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "# Get the keys of the saved state dictionary\n",
    "saved_keys = set(saved_state_dict.keys())\n",
    "\n",
    "# Get the keys of the current model state dictionary\n",
    "model_keys = set(inference_model.state_dict().keys())\n",
    "\n",
    "# Find missing keys\n",
    "missing_keys = model_keys - saved_keys\n",
    "\n",
    "# Create a new state dictionary by filtering out missing keys\n",
    "new_state_dict = {key: saved_state_dict[key] for key in saved_keys.intersection(model_keys)}\n",
    "\n",
    "# Load the new state dictionary into the model\n",
    "inference_model.load_state_dict(new_state_dict, strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.load('multi_head_with_pos_encod_weights_0_525000.pt',map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\basha\\Desktop\\founders_and_coders\\Untitled-1.ipynb Cell 5\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/basha/Desktop/founders_and_coders/Untitled-1.ipynb#W3sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m iterable_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(iterable_text)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/basha/Desktop/founders_and_coders/Untitled-1.ipynb#W3sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m iterable_tensor \u001b[39m=\u001b[39m iterable_tensor\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/basha/Desktop/founders_and_coders/Untitled-1.ipynb#W3sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m probability_matrix \u001b[39m=\u001b[39m inference_model(iterable_tensor)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/basha/Desktop/founders_and_coders/Untitled-1.ipynb#W3sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m#print(probability_matrix.size())\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/basha/Desktop/founders_and_coders/Untitled-1.ipynb#W3sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m probability_vector \u001b[39m=\u001b[39m probability_matrix[\u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n",
      "File \u001b[1;32mc:\\Users\\basha\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\basha\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\basha\\Desktop\\founders_and_coders\\Untitled-1.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/basha/Desktop/founders_and_coders/Untitled-1.ipynb#W3sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/basha/Desktop/founders_and_coders/Untitled-1.ipynb#W3sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m     positional_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpositional_embeddings(inputs)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/basha/Desktop/founders_and_coders/Untitled-1.ipynb#W3sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m     \u001b[39mfor\u001b[39;00m transformer_layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_layer: positional_embeddings  \u001b[39m=\u001b[39m transformer_layer(positional_embeddings)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/basha/Desktop/founders_and_coders/Untitled-1.ipynb#W3sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m     ff_norm_output \u001b[39m=\u001b[39m positional_embeddings\n",
      "File \u001b[1;32mc:\\Users\\basha\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\basha\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\basha\\Desktop\\founders_and_coders\\Untitled-1.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/basha/Desktop/founders_and_coders/Untitled-1.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/basha/Desktop/founders_and_coders/Untitled-1.ipynb#W3sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     input_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_embeddings(inputs)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/basha/Desktop/founders_and_coders/Untitled-1.ipynb#W3sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     positional_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_encoding(torch\u001b[39m.\u001b[39;49marange(inputs\u001b[39m.\u001b[39;49msize(\u001b[39m1\u001b[39;49m))\u001b[39m.\u001b[39;49mto(device))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/basha/Desktop/founders_and_coders/Untitled-1.ipynb#W3sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     positional_embeddings \u001b[39m=\u001b[39m input_embeddings \u001b[39m+\u001b[39m positional_embeddings\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/basha/Desktop/founders_and_coders/Untitled-1.ipynb#W3sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m positional_embeddings\n",
      "File \u001b[1;32mc:\\Users\\basha\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:289\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    285\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    286\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    287\u001b[0m     )\n\u001b[0;32m    288\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m\"\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 289\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    290\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    291\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m    292\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    293\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Import Libraries\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Importing the Transformer architecture\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Write the inference function\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "\n",
    "vocab_size = 16000\n",
    "input_embedding_dimensions = 512\n",
    "max_sequence_length = 1194\n",
    "ff_dimension = 2048\n",
    "num_heads = 1\n",
    "\n",
    "# Instantiate the model\n",
    "# Step 3: Load the pre-trained weights\n",
    "inference_model.load_state_dict(g['model_state_dict'])\n",
    "# Set the model to evaluation mode (important for models with batch normalization and dropout)\n",
    "inference_model.eval()\n",
    "\n",
    "tk = spm.SentencePieceProcessor()\n",
    "tk.load('tiny_piece.model')\n",
    "\n",
    "tokenised_text = tk.EncodeAsIds('hello my name is')\n",
    "iterable_text = tokenised_text\n",
    "\n",
    "next_token_id = tk.PieceToId(\"[BOS]\")\n",
    "break_count = 0\n",
    "\n",
    "while (next_token_id != tk.PieceToId(\"[EOS]\")) & (break_count <= max_sequence_length):\n",
    "    iterable_tensor = torch.tensor(iterable_text)\n",
    "    iterable_tensor = iterable_tensor.view(1,-1)\n",
    "    \n",
    "    probability_matrix = inference_model(iterable_tensor)\n",
    "    #print(probability_matrix.size())\n",
    "    probability_vector = probability_matrix[0, -1, :]\n",
    "    #print(probability_vector.size())\n",
    "    next_token_id = (torch.argmax(probability_vector))\n",
    "    #print(next_token_id)\n",
    "    iterable_text = iterable_text + [next_token_id.item()]\n",
    "    #print(tk.decode(iterable_text))\n",
    "    break_count += 1\n",
    "    \n",
    "final_text = tk.decode(iterable_text)\n",
    "print(final_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
