{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MNIST\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_image, make_grid\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manimation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FuncAnimation, PillowWriter\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py:56\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcycler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cycler\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolorbar\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/colorbar.py:19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, cbook, collections, cm, colors, contour, ticker\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martist\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmartist\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatches\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpatches\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/collections.py:20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (_api, _path, artist, cbook, cm, colors \u001b[38;5;28;01mas\u001b[39;00m mcolors, _docstring,\n\u001b[1;32m     21\u001b[0m                hatch \u001b[38;5;28;01mas\u001b[39;00m mhatch, lines \u001b[38;5;28;01mas\u001b[39;00m mlines, path \u001b[38;5;28;01mas\u001b[39;00m mpath, transforms)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_enums\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JoinStyle, CapStyle\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# \"color\" is excluded; it is a compound setter, and its docstring differs\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# in LineCollection.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/lines.py:1467\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;124;03m        Return whether line has a dashed linestyle.\u001b[39;00m\n\u001b[1;32m   1458\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;124;03m        See also `~.Line2D.set_linestyle`.\u001b[39;00m\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[1;32m   1464\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_linestyle \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1467\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAxLine\u001b[39;00m(Line2D):\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;124;03m    A helper class that implements `~.Axes.axline`, by recomputing the artist\u001b[39;00m\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;124;03m    transform at draw time.\u001b[39;00m\n\u001b[1;32m   1471\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1473\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, xy1, xy2, slope, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/artist.py:150\u001b[0m, in \u001b[0;36mArtist.__init_subclass__\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 150\u001b[0m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_set_signature_and_docstring\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/artist.py:178\u001b[0m, in \u001b[0;36mArtist._update_set_signature_and_docstring\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset\u001b[38;5;241m.\u001b[39m__signature__ \u001b[38;5;241m=\u001b[39m Signature(\n\u001b[1;32m    169\u001b[0m     [Parameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, Parameter\u001b[38;5;241m.\u001b[39mPOSITIONAL_OR_KEYWORD),\n\u001b[1;32m    170\u001b[0m      \u001b[38;5;241m*\u001b[39m[Parameter(prop, Parameter\u001b[38;5;241m.\u001b[39mKEYWORD_ONLY, default\u001b[38;5;241m=\u001b[39m_UNSET)\n\u001b[1;32m    171\u001b[0m        \u001b[38;5;28;01mfor\u001b[39;00m prop \u001b[38;5;129;01min\u001b[39;00m ArtistInspector(\u001b[38;5;28mcls\u001b[39m)\u001b[38;5;241m.\u001b[39mget_setters()\n\u001b[1;32m    172\u001b[0m        \u001b[38;5;28;01mif\u001b[39;00m prop \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m Artist\u001b[38;5;241m.\u001b[39m_PROPERTIES_EXCLUDED_FROM_SET]])\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset\u001b[38;5;241m.\u001b[39m_autogenerated_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSet multiple properties at once.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSupported properties are\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[43mkwdoc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/artist.py:1856\u001b[0m, in \u001b[0;36mkwdoc\u001b[0;34m(artist)\u001b[0m\n\u001b[1;32m   1838\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1839\u001b[0m \u001b[38;5;124;03mInspect an `~matplotlib.artist.Artist` class (using `.ArtistInspector`) and\u001b[39;00m\n\u001b[1;32m   1840\u001b[0m \u001b[38;5;124;03mreturn information about its settable properties and their current values.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1851\u001b[0m \u001b[38;5;124;03m    use in Sphinx) if it is True.\u001b[39;00m\n\u001b[1;32m   1852\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1853\u001b[0m ai \u001b[38;5;241m=\u001b[39m ArtistInspector(artist)\n\u001b[1;32m   1854\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ai\u001b[38;5;241m.\u001b[39mpprint_setters_rest(leadingspace\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m   1855\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocstring.hardcopy\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[0;32m-> 1856\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProperties:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpprint_setters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleadingspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/artist.py:1617\u001b[0m, in \u001b[0;36mArtistInspector.pprint_setters\u001b[0;34m(self, prop, leadingspace)\u001b[0m\n\u001b[1;32m   1615\u001b[0m lines \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1616\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prop \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_setters()):\n\u001b[0;32m-> 1617\u001b[0m     accepts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_valid_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1618\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maliased_name(prop)\n\u001b[1;32m   1619\u001b[0m     lines\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpad\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccepts\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/artist.py:1484\u001b[0m, in \u001b[0;36mArtistInspector.get_valid_values\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m   1481\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1482\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo, name)\n\u001b[0;32m-> 1484\u001b[0m docstring \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetdoc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m docstring \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:737\u001b[0m, in \u001b[0;36mgetdoc\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 737\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43m_finddoc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m    739\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:664\u001b[0m, in \u001b[0;36m_finddoc\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_finddoc\u001b[39m(obj):\n\u001b[0;32m--> 664\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43misclass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    665\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m base \u001b[38;5;129;01min\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__mro__\u001b[39m:\n\u001b[1;32m    666\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m base \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:191\u001b[0m, in \u001b[0;36misclass\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return true if the object is a module.\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m    Module objects provide these attributes:\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m        __cached__      pathname to byte compiled file\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m        __doc__         documentation string\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03m        __file__        filename (missing for built-in modules)\"\"\"\u001b[39;00m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mobject\u001b[39m, types\u001b[38;5;241m.\u001b[39mModuleType)\n\u001b[0;32m--> 191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21misclass\u001b[39m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m    192\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return true if the object is a class.\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m    Class objects provide these attributes:\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m        __doc__         documentation string\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m        __module__      name of module in which this class was defined\"\"\"\u001b[39;00m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mobject\u001b[39m, \u001b[38;5;28mtype\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "''' \n",
    "This script does conditional image generation on MNIST, using a diffusion model\n",
    "\n",
    "This code is modified from,\n",
    "https://github.com/cloneofsimo/minDiffusion\n",
    "\n",
    "Diffusion model is based on DDPM,\n",
    "https://arxiv.org/abs/2006.11239\n",
    "\n",
    "The conditioning idea is taken from 'Classifier-Free Diffusion Guidance',\n",
    "https://arxiv.org/abs/2207.12598\n",
    "\n",
    "This technique also features in ImageGen 'Photorealistic Text-to-Image Diffusion Modelswith Deep Language Understanding',\n",
    "https://arxiv.org/abs/2205.11487\n",
    "\n",
    "'''\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "import numpy as np\n",
    "import matplot_dataset\n",
    "\n",
    "class ResidualConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, out_channels: int, is_res: bool = False\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        '''\n",
    "        standard ResNet style convolutional block\n",
    "        '''\n",
    "        self.same_channels = in_channels==out_channels\n",
    "        self.is_res = is_res\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.is_res:\n",
    "            x1 = self.conv1(x)\n",
    "            x2 = self.conv2(x1)\n",
    "            # this adds on correct residual in case channels have increased\n",
    "            if self.same_channels:\n",
    "                out = x + x2\n",
    "            else:\n",
    "                out = x1 + x2 \n",
    "            return out / 1.414\n",
    "        else:\n",
    "            x1 = self.conv1(x)\n",
    "            x2 = self.conv2(x1)\n",
    "            return x2\n",
    "\n",
    "\n",
    "class UnetDown(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetDown, self).__init__()\n",
    "        '''\n",
    "        process and downscale the image feature maps\n",
    "        '''\n",
    "        layers = [ResidualConvBlock(in_channels, out_channels), nn.MaxPool2d(2)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class UnetUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetUp, self).__init__()\n",
    "        '''\n",
    "        process and upscale the image feature maps\n",
    "        '''\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, 2, 2),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = torch.cat((x, skip), 1)\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EmbedFC(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim):\n",
    "        super(EmbedFC, self).__init__()\n",
    "        '''\n",
    "        generic one layer FC NN for embedding things  \n",
    "        '''\n",
    "        self.input_dim = input_dim\n",
    "        layers = [\n",
    "            nn.Linear(input_dim, emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class ContextUnet(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat = 256, n_classes=10, image_height= 28):\n",
    "        super(ContextUnet, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
    "\n",
    "        self.down1 = UnetDown(n_feat, n_feat)\n",
    "        self.down2 = UnetDown(n_feat, 2 * n_feat)\n",
    "        self.image_pooling_dimension = int(image_height/4)\n",
    "        self.to_vec = nn.Sequential(nn.AvgPool2d(self.image_pooling_dimension), nn.GELU())\n",
    "\n",
    "        self.timeembed1 = EmbedFC(1, 2*n_feat)\n",
    "        self.timeembed2 = EmbedFC(1, 1*n_feat)\n",
    "        self.contextembed1 = EmbedFC(n_classes, 2*n_feat)\n",
    "        self.contextembed2 = EmbedFC(n_classes, 1*n_feat)\n",
    "\n",
    "        self.up0 = nn.Sequential(\n",
    "            # nn.ConvTranspose2d(6 * n_feat, 2 * n_feat, 7, 7), # when concat temb and cemb end up w 6*n_feat\n",
    "            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, self.image_pooling_dimension,self.image_pooling_dimension), # otherwise just have 2*n_feat\n",
    "            nn.GroupNorm(8, 2 * n_feat),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
    "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1),\n",
    "            nn.GroupNorm(8, n_feat),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, c, t, context_mask):\n",
    "        # x is (noisy) image, c is context label, t is timestep, \n",
    "        # context_mask says which samples to block the context on\n",
    "\n",
    "        x = self.init_conv(x)\n",
    "        down1 = self.down1(x)\n",
    "        down2 = self.down2(down1)\n",
    "        hiddenvec = self.to_vec(down2)\n",
    "\n",
    "        # convert context to one hot embedding\n",
    "        c = nn.functional.one_hot(c, num_classes=self.n_classes).type(torch.float)\n",
    "        # mask out context if context_mask == 1\n",
    "        context_mask = context_mask[:, None]\n",
    "        context_mask = context_mask.repeat(1,self.n_classes)\n",
    "        context_mask = (-1*(1-context_mask)) # need to flip 0 <-> 1\n",
    "        c = c * context_mask\n",
    "        # embed context, time step\n",
    "        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)\n",
    "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
    "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n",
    "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
    "        # could concatenate the context embedding here instead of adaGN\n",
    "        # hiddenvec = torch.cat((hiddenvec, temb1, cemb1), 1)\n",
    "\n",
    "        up1 = self.up0(hiddenvec)\n",
    "        # up2 = self.up1(up1, down2) # if want to avoid add and multiply embeddings\n",
    "        up2 = self.up1(cemb1*up1+ temb1, down2)  # add and multiply embeddings\n",
    "        up3 = self.up2(cemb2*up2+ temb2, down1)\n",
    "        out = self.out(torch.cat((up3, x), 1))\n",
    "        return out\n",
    "\n",
    "\n",
    "def ddpm_schedules(beta1, beta2, T):\n",
    "    \"\"\"\n",
    "    Returns pre-computed schedules for DDPM sampling, training process.\n",
    "    \"\"\"\n",
    "    assert beta1 < beta2 < 1.0, \"beta1 and beta2 must be in (0, 1)\"\n",
    "\n",
    "    beta_t = (beta2 - beta1) * torch.arange(0, T + 1, dtype=torch.float32) / T + beta1\n",
    "    sqrt_beta_t = torch.sqrt(beta_t)\n",
    "    alpha_t = 1 - beta_t\n",
    "    log_alpha_t = torch.log(alpha_t)\n",
    "    alphabar_t = torch.cumsum(log_alpha_t, dim=0).exp()\n",
    "\n",
    "    sqrtab = torch.sqrt(alphabar_t)\n",
    "    oneover_sqrta = 1 / torch.sqrt(alpha_t)\n",
    "\n",
    "    sqrtmab = torch.sqrt(1 - alphabar_t)\n",
    "    mab_over_sqrtmab_inv = (1 - alpha_t) / sqrtmab\n",
    "\n",
    "    return {\n",
    "        \"alpha_t\": alpha_t,  # \\alpha_t\n",
    "        \"oneover_sqrta\": oneover_sqrta,  # 1/\\sqrt{\\alpha_t}\n",
    "        \"sqrt_beta_t\": sqrt_beta_t,  # \\sqrt{\\beta_t}\n",
    "        \"alphabar_t\": alphabar_t,  # \\bar{\\alpha_t}\n",
    "        \"sqrtab\": sqrtab,  # \\sqrt{\\bar{\\alpha_t}}\n",
    "        \"sqrtmab\": sqrtmab,  # \\sqrt{1-\\bar{\\alpha_t}}\n",
    "        \"mab_over_sqrtmab\": mab_over_sqrtmab_inv,  # (1-\\alpha_t)/\\sqrt{1-\\bar{\\alpha_t}}\n",
    "    }\n",
    "\n",
    "\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(self, nn_model, betas, n_T, device, drop_prob=0.1):\n",
    "        super(DDPM, self).__init__()\n",
    "        self.nn_model = nn_model.to(device)\n",
    "\n",
    "        # register_buffer allows accessing dictionary produced by ddpm_schedules\n",
    "        # e.g. can access self.sqrtab later\n",
    "        for k, v in ddpm_schedules(betas[0], betas[1], n_T).items():\n",
    "            self.register_buffer(k, v)\n",
    "\n",
    "        self.n_T = n_T\n",
    "        self.device = device\n",
    "        self.drop_prob = drop_prob\n",
    "        self.loss_mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        \"\"\"\n",
    "        this method is used in training, so samples t and noise randomly\n",
    "        \"\"\"\n",
    "\n",
    "        _ts = torch.randint(1, self.n_T+1, (x.shape[0],)).to(self.device)  # t ~ Uniform(0, n_T)\n",
    "        noise = torch.randn_like(x)  # eps ~ N(0, 1)\n",
    "\n",
    "        x_t = (\n",
    "            self.sqrtab[_ts, None, None, None] * x\n",
    "            + self.sqrtmab[_ts, None, None, None] * noise\n",
    "        )  # This is the x_t, which is sqrt(alphabar) x_0 + sqrt(1-alphabar) * eps\n",
    "        # We should predict the \"error term\" from this x_t. Loss is what we return.\n",
    "\n",
    "        # dropout context with some probability\n",
    "        context_mask = torch.bernoulli(torch.zeros_like(c)+self.drop_prob).to(self.device)\n",
    "        # return MSE between added noise, and our predicted noise\n",
    "        return self.loss_mse(noise, self.nn_model(x_t, c, _ts / self.n_T, context_mask))\n",
    "\n",
    "    def sample(self, n_sample, size, device, guide_w = 0.0):\n",
    "        # we follow the guidance sampling scheme described in 'Classifier-Free Diffusion Guidance'\n",
    "        # to make the fwd passes efficient, we concat two versions of the dataset,\n",
    "        # one with context_mask=0 and the other context_mask=1\n",
    "        # we then mix the outputs with the guidance scale, w\n",
    "        # where w>0 means more guidance\n",
    "\n",
    "        x_i = torch.randn(n_sample, *size).to(device)  # x_T ~ N(0, 1), sample initial noise\n",
    "        c_i = torch.arange(0,10).to(device) # context for us just cycles throught the mnist labels\n",
    "        c_i = c_i.repeat(int(n_sample/c_i.shape[0]))\n",
    "\n",
    "        # don't drop context at test time\n",
    "        context_mask = torch.zeros_like(c_i).to(device)\n",
    "\n",
    "        # double the batch\n",
    "        c_i = c_i.repeat(2)\n",
    "        context_mask = context_mask.repeat(2)\n",
    "        context_mask[n_sample:] = 1. # makes second half of batch context free\n",
    "\n",
    "        x_i_store = [] # keep track of generated steps in case want to plot something \n",
    "        print()\n",
    "        for i in range(self.n_T, 0, -1):\n",
    "            print(f'sampling timestep {i}',end='\\r')\n",
    "            t_is = torch.tensor([i / self.n_T]).to(device)\n",
    "            t_is = t_is.repeat(n_sample,1,1,1)\n",
    "\n",
    "            # double batch\n",
    "            x_i = x_i.repeat(2,1,1,1)\n",
    "            t_is = t_is.repeat(2,1,1,1)\n",
    "\n",
    "            z = torch.randn(n_sample, *size).to(device) if i > 1 else 0\n",
    "\n",
    "            # split predictions and compute weighting\n",
    "            eps = self.nn_model(x_i, c_i, t_is, context_mask)\n",
    "            eps1 = eps[:n_sample]\n",
    "            eps2 = eps[n_sample:]\n",
    "            eps = (1+guide_w)*eps1 - guide_w*eps2\n",
    "            x_i = x_i[:n_sample]\n",
    "            x_i = (\n",
    "                self.oneover_sqrta[i] * (x_i - eps * self.mab_over_sqrtmab[i])\n",
    "                + self.sqrt_beta_t[i] * z\n",
    "            )\n",
    "            if i%20==0 or i==self.n_T or i<8:\n",
    "                x_i_store.append(x_i.detach().cpu().numpy())\n",
    "        \n",
    "        x_i_store = np.array(x_i_store)\n",
    "        return x_i, x_i_store\n",
    "\n",
    "def digit_to_word(digit):\n",
    "    word_dict = {\n",
    "        '0': 'zero',\n",
    "        '1': 'one',\n",
    "        '2': 'two',\n",
    "        '3': 'three',\n",
    "        '4': 'four',\n",
    "        '5': 'five',\n",
    "        '6': 'six',\n",
    "        '7': 'seven',\n",
    "        '8': 'eight',\n",
    "        '9': 'nine',\n",
    "    }\n",
    "\n",
    "    # Convert tensor elements to strings and map using the dictionary\n",
    "    return [word_dict.get(str(d.item()), 'unknown') for d in digit]\n",
    "\n",
    "\n",
    "def train_mnist():\n",
    "\n",
    "    # hardcoding these here\n",
    "    n_epoch = 1\n",
    "    batch_size =1\n",
    "    n_T = 400 # 500\n",
    "    device = \"cuda:0\"\n",
    "    n_classes = 10\n",
    "    n_feat = 256 # 128 ok, 256 better (but slower)\n",
    "    lrate = 1e-4\n",
    "    save_model = False\n",
    "    save_dir = './data/'\n",
    "    ws_test = [0.0, 0.5, 2.0] # strength of generative guidance\n",
    "    image_shape = (3,112,112)\n",
    "\n",
    "    ddpm = DDPM(nn_model=ContextUnet(in_channels=3, n_feat=n_feat, n_classes=n_classes, image_height = image_shape[-1]), betas=(1e-4, 0.02), n_T=n_T, device=device, drop_prob=0.1)\n",
    "    ddpm.to(device)\n",
    "\n",
    "    # optionally load a model\n",
    "    # ddpm.load_state_dict(torch.load(\"./data/diffusion_outputs/ddpm_unet01_mnist_9.pth\"))\n",
    "\n",
    "    tf = transforms.Compose([transforms.ToTensor()]) # mnist is already normalised 0 to 1\n",
    "\n",
    "    dataset = matplot_dataset.matplot_dataset(\"image_dataset\",image_height = image_shape[-1])\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=5)\n",
    "    optim = torch.optim.Adam(ddpm.parameters(), lr=lrate)\n",
    "\n",
    "    for ep in range(n_epoch):\n",
    "        print(f'epoch {ep}')\n",
    "        ddpm.train()\n",
    "\n",
    "        # linear lrate decay\n",
    "        optim.param_groups[0]['lr'] = lrate*(1-ep/n_epoch)\n",
    "\n",
    "        pbar = tqdm(dataloader)\n",
    "        loss_ema = None\n",
    "        for x, c in pbar:\n",
    "            optim.zero_grad()\n",
    "            x = x.to(device)\n",
    "            c = c.to(device)\n",
    "            loss = ddpm(x, c)\n",
    "            loss.backward()\n",
    "            if loss_ema is None:\n",
    "                loss_ema = loss.item()\n",
    "            else:\n",
    "                loss_ema = 0.95 * loss_ema + 0.05 * loss.item()\n",
    "            pbar.set_description(f\"loss: {loss_ema:.4f}\")\n",
    "            optim.step()\n",
    "        \n",
    "        # for eval, save an image of currently generated samples (top rows)\n",
    "        # followed by real images (bottom rows)\n",
    "        ddpm.eval()\n",
    "        with torch.no_grad():\n",
    "            n_sample = 4*n_classes\n",
    "            for w_i, w in enumerate(ws_test):\n",
    "                x_gen, x_gen_store = ddpm.sample(n_sample, image_shape, device, guide_w=w)\n",
    "\n",
    "                # append some real images at bottom, order by class also\n",
    "                x_real = torch.Tensor(x_gen.shape).to(device)\n",
    "                for k in range(n_classes):\n",
    "                    for j in range(int(n_sample/n_classes)):\n",
    "                        try: \n",
    "                            idx = torch.squeeze((c == k).nonzero())[j]\n",
    "                        except:\n",
    "                            idx = 0\n",
    "                        x_real[k+(j*n_classes)] = x[idx]\n",
    "\n",
    "                x_all = torch.cat([x_gen, x_real])\n",
    "                grid = make_grid(x_all*-1 + 1, nrow=10, padding=20, pad_value=1.0, scale_each = True)\n",
    "                save_image(grid, save_dir + f\"image_ep{ep}_w{w}.png\")\n",
    "                print('saved image at ' + save_dir + f\"image_ep{ep}_w{w}.png\")\n",
    "\n",
    "                if ep%5==0 or ep == int(n_epoch-1):\n",
    "                    # create gif of images evolving over time, based on x_gen_store\n",
    "                    fig, axs = plt.subplots(nrows=int(n_sample/n_classes), ncols=n_classes,sharex=True,sharey=True,figsize=(8,3))\n",
    "                    def animate_diff(i, x_gen_store):\n",
    "                        print(f'gif animating frame {i} of {x_gen_store.shape[0]}', end='\\r')\n",
    "                        plots = []\n",
    "                        for row in range(int(n_sample/n_classes)):\n",
    "                            for col in range(n_classes):\n",
    "                                axs[row, col].clear()\n",
    "                                axs[row, col].set_xticks([])\n",
    "                                axs[row, col].set_yticks([])\n",
    "                                # plots.append(axs[row, col].imshow(x_gen_store[i,(row*n_classes)+col,0],cmap='gray'))\n",
    "                                plots.append(axs[row, col].imshow(-x_gen_store[i,(row*n_classes)+col,0],vmin=(-x_gen_store[i]).min(), vmax=(-x_gen_store[i]).max()))\n",
    "                        return plots\n",
    "                    ani = FuncAnimation(fig, animate_diff, fargs=[x_gen_store],  interval=200, blit=False, repeat=True, frames=x_gen_store.shape[0])    \n",
    "                    ani.save(save_dir + f\"gif_ep{ep}_w{w}.gif\", dpi=100, writer=PillowWriter(fps=5))\n",
    "                    print('saved image at ' + save_dir + f\"gif_ep{ep}_w{w}.gif\")\n",
    "        # optionally save model\n",
    "        if save_model and ep == int(n_epoch-1):\n",
    "            torch.save(ddpm.state_dict(), save_dir + f\"model_{ep}.pth\")\n",
    "            print('saved model at ' + save_dir + f\"model_{ep}.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_mnist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = matplot_dataset.matplot_dataset(\"image_dataset\",image_height = 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "This script does conditional image generation on MNIST, using a diffusion model\n",
    "\n",
    "This code is modified from,\n",
    "https://github.com/cloneofsimo/minDiffusion\n",
    "\n",
    "Diffusion model is based on DDPM,\n",
    "https://arxiv.org/abs/2006.11239\n",
    "\n",
    "The conditioning idea is taken from 'Classifier-Free Diffusion Guidance',\n",
    "https://arxiv.org/abs/2207.12598\n",
    "\n",
    "This technique also features in ImageGen 'Photorealistic Text-to-Image Diffusion Modelswith Deep Language Understanding',\n",
    "https://arxiv.org/abs/2205.11487\n",
    "\n",
    "'''\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "import numpy as np\n",
    "import matplot_dataset\n",
    "\n",
    "class ResidualConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, out_channels: int, is_res: bool = False\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        '''\n",
    "        standard ResNet style convolutional block\n",
    "        '''\n",
    "        self.same_channels = in_channels==out_channels\n",
    "        self.is_res = is_res\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.is_res:\n",
    "            x1 = self.conv1(x)\n",
    "            x2 = self.conv2(x1)\n",
    "            # this adds on correct residual in case channels have increased\n",
    "            if self.same_channels:\n",
    "                out = x + x2\n",
    "            else:\n",
    "                out = x1 + x2 \n",
    "            return out / 1.414\n",
    "        else:\n",
    "            x1 = self.conv1(x)\n",
    "            x2 = self.conv2(x1)\n",
    "            return x2\n",
    "\n",
    "\n",
    "class UnetDown(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetDown, self).__init__()\n",
    "        '''\n",
    "        process and downscale the image feature maps\n",
    "        '''\n",
    "        layers = [ResidualConvBlock(in_channels, out_channels), nn.MaxPool2d(1)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class UnetUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetUp, self).__init__()\n",
    "        '''\n",
    "        process and upscale the image feature maps\n",
    "        '''\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, 2, 2),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = torch.cat((x, skip), 1)\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EmbedFC(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim):\n",
    "        super(EmbedFC, self).__init__()\n",
    "        '''\n",
    "        generic one layer FC NN for embedding things  \n",
    "        '''\n",
    "        self.input_dim = input_dim\n",
    "        layers = [\n",
    "            nn.Linear(input_dim, emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class ContextUnet(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat = 256, n_classes=10, image_height= 28):\n",
    "        super(ContextUnet, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
    "\n",
    "        self.down1 = UnetDown(n_feat, n_feat)\n",
    "        self.down2 = UnetDown(n_feat, 2 * n_feat)\n",
    "        self.image_pooling_dimension = int(image_height/1)\n",
    "        self.to_vec = nn.Sequential(nn.AvgPool2d(self.image_pooling_dimension), nn.GELU())\n",
    "\n",
    "        self.timeembed1 = EmbedFC(1, 2*n_feat)\n",
    "        self.timeembed2 = EmbedFC(1, 1*n_feat)\n",
    "        self.contextembed1 = EmbedFC(n_classes, 2*n_feat)\n",
    "        self.contextembed2 = EmbedFC(n_classes, 1*n_feat)\n",
    "\n",
    "\n",
    "        self.up0 = nn.Sequential(\n",
    "            # nn.ConvTranspose2d(6 * n_feat, 2 * n_feat, 7, 7), # when concat temb and cemb end up w 6*n_feat\n",
    "            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, self.image_pooling_dimension,self.image_pooling_dimension), # otherwise just have 2*n_feat\n",
    "            nn.GroupNorm(17, 2 * n_feat),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
    "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1),\n",
    "            nn.GroupNorm(17, n_feat),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, c, t, context_mask):\n",
    "        # x is (noisy) image, c is context label, t is timestep, \n",
    "        # context_mask says which samples to block the context on\n",
    "\n",
    "        x = self.init_conv(x)\n",
    "        down1 = self.down1(x)\n",
    "        down2 = self.down2(down1)\n",
    "        hiddenvec = self.to_vec(down2)\n",
    "\n",
    "        # convert context to one hot embedding\n",
    "        c = nn.functional.one_hot(c, num_classes=self.n_classes).type(torch.float)\n",
    "        # mask out context if context_mask == 1\n",
    "        context_mask = context_mask[:, None]\n",
    "        context_mask = context_mask.repeat(1,self.n_classes)\n",
    "        context_mask = (-1*(1-context_mask)) # need to flip 0 <-> 1\n",
    "        c = c * context_mask\n",
    "        # embed context, time step\n",
    "        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)\n",
    "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
    "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n",
    "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
    "        # could concatenate the context embedding here instead of adaGN\n",
    "        # hiddenvec = torch.cat((hiddenvec, temb1, cemb1), 1)\n",
    "\n",
    "        up1 = self.up0(hiddenvec)\n",
    "        # up2 = self.up1(up1, down2) # if want to avoid add and multiply embeddings\n",
    "        up2 = self.up1(cemb1*up1+ temb1, down2)  # add and multiply embeddings\n",
    "        up3 = self.up2(cemb2*up2+ temb2, down1)\n",
    "        out = self.out(torch.cat((up3, x), 1))\n",
    "        return out\n",
    "\n",
    "\n",
    "def ddpm_schedules(beta1, beta2, T):\n",
    "    \"\"\"\n",
    "    Returns pre-computed schedules for DDPM sampling, training process.\n",
    "    \"\"\"\n",
    "    assert beta1 < beta2 < 1.0, \"beta1 and beta2 must be in (0, 1)\"\n",
    "\n",
    "    beta_t = (beta2 - beta1) * torch.arange(0, T + 1, dtype=torch.float32) / T + beta1\n",
    "    sqrt_beta_t = torch.sqrt(beta_t)\n",
    "    alpha_t = 1 - beta_t\n",
    "    log_alpha_t = torch.log(alpha_t)\n",
    "    alphabar_t = torch.cumsum(log_alpha_t, dim=0).exp()\n",
    "\n",
    "    sqrtab = torch.sqrt(alphabar_t)\n",
    "    oneover_sqrta = 1 / torch.sqrt(alpha_t)\n",
    "\n",
    "    sqrtmab = torch.sqrt(1 - alphabar_t)\n",
    "    mab_over_sqrtmab_inv = (1 - alpha_t) / sqrtmab\n",
    "\n",
    "    return {\n",
    "        \"alpha_t\": alpha_t,  # \\alpha_t\n",
    "        \"oneover_sqrta\": oneover_sqrta,  # 1/\\sqrt{\\alpha_t}\n",
    "        \"sqrt_beta_t\": sqrt_beta_t,  # \\sqrt{\\beta_t}\n",
    "        \"alphabar_t\": alphabar_t,  # \\bar{\\alpha_t}\n",
    "        \"sqrtab\": sqrtab,  # \\sqrt{\\bar{\\alpha_t}}\n",
    "        \"sqrtmab\": sqrtmab,  # \\sqrt{1-\\bar{\\alpha_t}}\n",
    "        \"mab_over_sqrtmab\": mab_over_sqrtmab_inv,  # (1-\\alpha_t)/\\sqrt{1-\\bar{\\alpha_t}}\n",
    "    }\n",
    "\n",
    "\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(self, nn_model, betas, n_T, device, drop_prob=0.1):\n",
    "        super(DDPM, self).__init__()\n",
    "        self.nn_model = nn_model.to(device)\n",
    "\n",
    "        # register_buffer allows accessing dictionary produced by ddpm_schedules\n",
    "        # e.g. can access self.sqrtab later\n",
    "        for k, v in ddpm_schedules(betas[0], betas[1], n_T).items():\n",
    "            self.register_buffer(k, v)\n",
    "\n",
    "        self.n_T = n_T\n",
    "        self.device = device\n",
    "        self.drop_prob = drop_prob\n",
    "        self.loss_mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        \"\"\"\n",
    "        this method is used in training, so samples t and noise randomly\n",
    "        \"\"\"\n",
    "\n",
    "        _ts = torch.randint(1, self.n_T+1, (x.shape[0],)).to(self.device)  # t ~ Uniform(0, n_T)\n",
    "        noise = torch.randn_like(x)  # eps ~ N(0, 1)\n",
    "\n",
    "        x_t = (\n",
    "            self.sqrtab[_ts, None, None, None] * x\n",
    "            + self.sqrtmab[_ts, None, None, None] * noise\n",
    "        )  # This is the x_t, which is sqrt(alphabar) x_0 + sqrt(1-alphabar) * eps\n",
    "        # We should predict the \"error term\" from this x_t. Loss is what we return.\n",
    "\n",
    "        # dropout context with some probability\n",
    "        context_mask = torch.bernoulli(torch.zeros_like(c)+self.drop_prob).to(self.device)\n",
    "        # return MSE between added noise, and our predicted noise\n",
    "        return self.loss_mse(noise, self.nn_model(x_t, c, _ts / self.n_T, context_mask))\n",
    "\n",
    "    def sample(self, n_sample, size, device, guide_w = 0.0):\n",
    "        # we follow the guidance sampling scheme described in 'Classifier-Free Diffusion Guidance'\n",
    "        # to make the fwd passes efficient, we concat two versions of the dataset,\n",
    "        # one with context_mask=0 and the other context_mask=1\n",
    "        # we then mix the outputs with the guidance scale, w\n",
    "        # where w>0 means more guidance\n",
    "\n",
    "        x_i = torch.randn(n_sample, *size).to(device)  # x_T ~ N(0, 1), sample initial noise\n",
    "        c_i = torch.arange(0,10).to(device) # context for us just cycles throught the mnist labels\n",
    "        c_i = c_i.repeat(int(n_sample/c_i.shape[0]))\n",
    "\n",
    "        # don't drop context at test time\n",
    "        context_mask = torch.zeros_like(c_i).to(device)\n",
    "\n",
    "        # double the batch\n",
    "        c_i = c_i.repeat(2)\n",
    "        context_mask = context_mask.repeat(2)\n",
    "        context_mask[n_sample:] = 1. # makes second half of batch context free\n",
    "\n",
    "        x_i_store = [] # keep track of generated steps in case want to plot something \n",
    "        print()\n",
    "        for i in range(self.n_T, 0, -1):\n",
    "            print(f'sampling timestep {i}',end='\\r')\n",
    "            t_is = torch.tensor([i / self.n_T]).to(device)\n",
    "            t_is = t_is.repeat(n_sample,1,1,1)\n",
    "\n",
    "            # double batch\n",
    "            x_i = x_i.repeat(2,1,1,1)\n",
    "            t_is = t_is.repeat(2,1,1,1)\n",
    "\n",
    "            z = torch.randn(n_sample, *size).to(device) if i > 1 else 0\n",
    "\n",
    "            # split predictions and compute weighting\n",
    "            eps = self.nn_model(x_i, c_i, t_is, context_mask)\n",
    "            eps1 = eps[:n_sample]\n",
    "            eps2 = eps[n_sample:]\n",
    "            eps = (1+guide_w)*eps1 - guide_w*eps2\n",
    "            x_i = x_i[:n_sample]\n",
    "            x_i = (\n",
    "                self.oneover_sqrta[i] * (x_i - eps * self.mab_over_sqrtmab[i])\n",
    "                + self.sqrt_beta_t[i] * z\n",
    "            )\n",
    "            if i%20==0 or i==self.n_T or i<8:\n",
    "                x_i_store.append(x_i.detach().cpu().numpy())\n",
    "        \n",
    "        x_i_store = np.array(x_i_store)\n",
    "        return x_i, x_i_store\n",
    "\n",
    "def digit_to_word(digit):\n",
    "    word_dict = {\n",
    "        '0': 'zero',\n",
    "        '1': 'one',\n",
    "        '2': 'two',\n",
    "        '3': 'three',\n",
    "        '4': 'four',\n",
    "        '5': 'five',\n",
    "        '6': 'six',\n",
    "        '7': 'seven',\n",
    "        '8': 'eight',\n",
    "        '9': 'nine',\n",
    "    }\n",
    "\n",
    "    # Convert tensor elements to strings and map using the dictionary\n",
    "    return [word_dict.get(str(d.item()), 'unknown') for d in digit]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 1\n",
    "batch_size =1\n",
    "n_T = 400 # 500\n",
    "device = \"cuda:0\"\n",
    "n_classes = 10\n",
    "n_feat = 51 # 128 ok, 256 better (but slower)\n",
    "lrate = 1e-4\n",
    "save_model = False\n",
    "save_dir = './data/'\n",
    "ws_test = [0.0, 0.5, 2.0] # strength of generative guidance\n",
    "image_shape = (3,60,60)\n",
    "\n",
    "ddpm = DDPM(nn_model=ContextUnet(in_channels=3, n_feat=n_feat, n_classes=n_classes, image_height = image_shape[-1]), betas=(1e-4, 0.02), n_T=n_T, device=device, drop_prob=0.1)\n",
    "ddpm.to(device)\n",
    "\n",
    "# optionally load a model\n",
    "# ddpm.load_state_dict(torch.load(\"./data/diffusion_outputs/ddpm_unet01_mnist_9.pth\"))\n",
    "\n",
    "tf = transforms.Compose([transforms.ToTensor()]) # mnist is already normalised 0 to 1\n",
    "\n",
    "dataset = matplot_dataset.matplot_dataset(\"image_dataset\",image_height = image_shape[-1])\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=5)\n",
    "optim = torch.optim.Adam(ddpm.parameters(), lr=lrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([7])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,c = dataset[2]\n",
    "c = torch.tensor([c])\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15000 [05:52<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "loss_ema = None\n",
    "pbar = tqdm(dataloader)\n",
    "optim.zero_grad()\n",
    "x = x.to(device)\n",
    "c = c.to(device)\n",
    "loss = ddpm(x, c)\n",
    "loss.backward()\n",
    "if loss_ema is None:\n",
    "    loss_ema = loss.item()\n",
    "else:\n",
    "    loss_ema = 0.95 * loss_ema + 0.05 * loss.item()\n",
    "pbar.set_description(f\"loss: {loss_ema:.4f}\")\n",
    "optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
