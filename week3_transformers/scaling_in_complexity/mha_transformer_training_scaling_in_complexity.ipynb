{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "import dataset_bes_train\n",
    "import dataset_bes_val\n",
    "import tokenizer_bes\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import bash_gpt\n",
    "import time\n",
    "from torch.autograd import profiler\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/bash1989/.cache/huggingface/datasets/roneneldan___parquet/roneneldan--TinyStories-6ac769f186d7da53/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d064cb0fd404feab14ae83244ae21be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/bash1989/.cache/huggingface/datasets/roneneldan___parquet/roneneldan--TinyStories-6ac769f186d7da53/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b4e5fb9a234029958742685dc21720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tk = (tokenizer_bes.TinyTokenizer()).load()\n",
    "ds = dataset_bes_train.TinyDataset()\n",
    "val_ds = dataset_bes_val.TinyDataset()\n",
    "batch_size = 4\n",
    "dl = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=True, collate_fn=ds.collate_fn)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, shuffle=True, collate_fn=ds.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = 'checkpoints/'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transformer_model = bash_gpt.GPT().to(device)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "lr = 3e-4\n",
    "optimizer = torch.optim.Adam(transformer_model.parameters(), lr=lr, eps=1e-9, betas=(0.9,.98))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26468480"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in transformer_model.parameters())\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "checkpoint_interval = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# wandb.login(key=\"9091f9245cf64052fa6d4eae03190a076fd87fe8\")\n",
    "# wandb.init(name = f'{transformer_model.num_heads} Head(s), params = {total_params:,}',project ='mlx_transformer_GPT', entity = 'basharkabalan',     config={\n",
    "#     \"learning_rate\": lr,\n",
    "#     \"architecture\": \"Trans_Bash\",\n",
    "#     \"dataset\": \"roneneldan/TinyStories\",\n",
    "#     \"epochs\": num_epochs,\n",
    "#     })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_at = wandb.Artifact(\"test_samples_\" + str(wandb.run.id), type=\"predictions\")\n",
    "# columns=[\"id\", \"word\", \"truth\", \"guess\"]\n",
    "# test_table = wandb.Table(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "writer = SummaryWriter(\"logs/profile_example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2023-11-15 07:19:55 2770:2770 init.cpp:155] function cbapi->getCuptiStatus() failed with error CUPTI_ERROR_NOT_INITIALIZED (15)\n",
      "WARNING:2023-11-15 07:19:55 2770:2770 init.cpp:156] CUPTI initialization failed - CUDA profiler activities will be missing\n",
      "INFO:2023-11-15 07:19:55 2770:2770 init.cpp:158] If you see CUPTI_ERROR_INSUFFICIENT_PRIVILEGES, refer to https://developer.nvidia.com/nvidia-development-tools-solutions-err-nvgpuctrperm-cupti\n",
      "STAGE:2023-11-15 07:19:55 2770:2770 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 9.8062\n",
      "train_loss: 8.4662, val_loss: 8.4662, val_acc: 0.1953\n",
      "Epoch 1/5, val_acc: 0.1953125, val_loss: 8.466161727905273\n",
      "train_loss: 8.4975\n",
      "train_loss: 6.8711, val_loss: 6.8711, val_acc: 0.3008\n",
      "Epoch 2/5, val_acc: 0.30078125, val_loss: 6.871096134185791\n",
      "train_loss: 6.9025\n",
      "train_loss: 6.8625, val_loss: 6.8625, val_acc: 0.2718\n",
      "Epoch 3/5, val_acc: 0.27184465527534485, val_loss: 6.862497329711914\n",
      "train_loss: 7.4318\n",
      "train_loss: 7.9319, val_loss: 7.9319, val_acc: 0.1189\n",
      "Epoch 4/5, val_acc: 0.11887254565954208, val_loss: 7.931860446929932\n",
      "train_loss: 6.7463\n",
      "train_loss: 5.7020, val_loss: 5.7020, val_acc: 0.3442\n",
      "Epoch 5/5, val_acc: 0.34421366453170776, val_loss: 5.70197057723999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-11-15 07:20:07 2770:2770 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2023-11-15 07:20:07 2770:2770 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::copy_        60.96%        3.523s        60.96%        3.523s       3.051ms        3.483s        60.24%        3.483s       3.016ms          1155  \n",
      "                               Optimizer.step#Adam.step         0.47%      27.063ms         5.36%     309.832ms      61.966ms      12.176ms         0.21%     406.252ms      81.250ms             5  \n",
      "enumerate(DataLoader)#_SingleProcessDataLoaderIter._...         3.22%     186.223ms         6.74%     389.801ms      38.980ms     182.594ms         3.16%     395.993ms      39.599ms            10  \n",
      "                                         aten::randperm         3.30%     190.681ms         6.60%     381.309ms      19.065ms     191.587ms         3.31%     384.211ms      19.211ms            20  \n",
      "                                        aten::embedding         0.03%       1.459ms         5.16%     297.982ms      14.899ms       1.288ms         0.02%     299.588ms      14.979ms            20  \n",
      "                                     aten::index_select         5.11%     295.377ms         5.13%     296.267ms      14.813ms     295.611ms         5.11%     297.728ms      14.886ms            20  \n",
      "    autograd::engine::evaluate_function: AddmmBackward0         0.11%       6.295ms         2.31%     133.645ms       2.056ms       1.294ms         0.02%     237.668ms       3.656ms            65  \n",
      "                                           aten::linear         0.38%      21.687ms         1.78%     103.148ms     793.446us       4.578ms         0.08%     230.481ms       1.773ms           130  \n",
      "                                         AddmmBackward0         0.53%      30.412ms         1.71%      98.560ms       1.516ms       4.859ms         0.08%     210.052ms       3.232ms            65  \n",
      "                                               aten::mm         0.74%      42.510ms         0.74%      42.510ms     327.000us     201.373ms         3.48%     201.373ms       1.549ms           130  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 5.780s\n",
      "Self CUDA time total: 5.782s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with profiler.profile(record_shapes=True, use_cuda=True) as prof:\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        transformer_model.train()\n",
    "        for idx, batch in enumerate(dl):\n",
    "    #         start_time = time.time()\n",
    "\n",
    "            tokens = batch['input'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output= transformer_model(tokens)\n",
    "            model_output = output.view(-1, output.size(-1))  # Reshape to [batch_size * seq_length, num_classes]\n",
    "            true_labels = labels.view(-1)  # Reshape to [batch_size * seq_length]\n",
    "            loss = loss_function(model_output, true_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    #         print(time.time()-start_time)\n",
    "            if idx % 500 == 0:\n",
    "                    print(f\"train_loss: {loss:.4f}\")\n",
    "    #                 wandb.log({\"train_loss\": loss})\n",
    "                    break\n",
    "            if idx % 5000 == 0: torch.save(transformer_model.state_dict(), f\"multi_head_with_pos_encod_weights_{epoch}_{idx}.pt\")\n",
    "\n",
    "\n",
    "\n",
    "        transformer_model.eval()\n",
    "        val_correct_predictions = torch.tensor(0)\n",
    "        val_correct_predictions = val_correct_predictions.to(device)\n",
    "        val_total_predictions = torch.tensor(0)\n",
    "        val_total_predictions = val_total_predictions.to(device)\n",
    "        with torch.no_grad():\n",
    "            for val_idx, val_batch in enumerate(val_dl):\n",
    "                val_tokens = val_batch['input'].to(device)\n",
    "                val_labels = val_batch['label'].to(device)\n",
    "                val_output= transformer_model(val_tokens)\n",
    "                val_model_output = val_output.view(-1, val_output.size(-1))  # Reshape to [batch_size * seq_length, num_classes]\n",
    "                val_true_labels = val_labels.view(-1)  # Reshape to [batch_size * seq_length]\n",
    "                val_loss = loss_function(val_model_output, val_true_labels)\n",
    "                val_max_indices = torch.argmax(val_model_output, dim=1)\n",
    "                val_correct_predictions += ((val_max_indices - val_true_labels)==0).sum()\n",
    "                val_total_predictions += len(val_true_labels)\n",
    "\n",
    "                if idx % 500 == 0:\n",
    "\n",
    "                    val_acc = val_correct_predictions/val_total_predictions\n",
    "                    print(f\"train_loss: {val_loss:.4f}, val_loss: {val_loss:.4f}, val_acc: {val_acc:.4f}\")\n",
    "                    break\n",
    "    #                 wandb.log({\"train_loss\": loss,\"val_acc\": val_acc, \"val_loss\": val_loss})\n",
    "                    # wandb.run.log_artifact(test_data_at)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # if idx%100 == 0:\n",
    "            #     test_table.add_data(tokens[0][3],  tk.decode(tokens[0][3].item()), tk.decode(true_labels[3].item()),  tk.decode(max_indices[3].item()))\n",
    "            #     test_data_at.add(test_table, \"predictions\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if (epoch + 1) % checkpoint_interval == 0:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch+1}.pt')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': transformer_model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss\n",
    "            }, checkpoint_path)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, val_acc: {val_acc}, val_loss: {val_loss.item()}\")\n",
    "\n",
    "    \n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "# # writer.add_text('Profiler Info', prof.key_averages().table(sort_by='cuda_time_total'))\n",
    "# writer.add_text('Profiler Info', prof.key_averages().table(sort_by='cuda_time_total'))\n",
    "# writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9d08191ac634e3d9\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9d08191ac634e3d9\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/profile_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
